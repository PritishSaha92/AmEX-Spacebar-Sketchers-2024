{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c3173b-b24a-461b-a48e-c9a4f5ddb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "# pip install tqdm\n",
    "# pip install ipywidgets\n",
    "# pip install \"numpy<2.0\"\n",
    "# pip install --upgrade scipy scikit-learn\n",
    "# pip install pyarrow\n",
    "# pip install lightgbm\n",
    "# pip install openpyxl\n",
    "# pip install optuna\n",
    "# pip install \\\n",
    "#     --extra-index-url=https://pypi.nvidia.com \\\n",
    "#     \"cudf-cu12==25.6.\" \"dask-cudf-cu12==25.6.\" \"cuml-cu12==25.6.*\" \\\n",
    "#     \"cugraph-cu12==25.6.\" \"nx-cugraph-cu12==25.6.\" \"cuxfilter-cu12==25.6.*\" \\\n",
    "#     \"cucim-cu12==25.6.\" \"pylibraft-cu12==25.6.\" \"raft-dask-cu12==25.6.*\" \\\n",
    "#     \"cuvs-cu12==25.6.\" \"nx-cugraph-cu12==25.6.\"\n",
    "# curl -L https://lambdalabs-guest-agent.s3.us-west-2.amazonaws.com/scripts/install.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f185bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Advanced Feature Engineering Pipeline (v3) ---\n",
      "1/7: Loading all raw data files...\n",
      "2/7: Processing timestamps...\n",
      "3/7: Engineering static offer features (including NLP)...\n",
      "  -> Generating NLP features from offer text...\n",
      "4/7: Engineering customer-offer historical interaction features (impressions, clicks & timing)...\n",
      "  -> Generating advanced temporal (time-since-last-event) features...\n",
      "5/7: Engineering POINT-IN-TIME customer transaction features to prevent data leakage...\n",
      "  -> Calculating point-in-time cumulative aggregates...\n",
      "  -> Merging point-in-time transaction features into main dataset...\n",
      "6/7: Finalizing datasets and saving to disk...\n",
      "  -> Filling NaN values introduced during merges...\n",
      "\n",
      "--- Advanced Feature Engineering (v3) Complete! ---\n",
      "✅ Saved 'inter/train_enriched.parquet'\n",
      "✅ Saved 'inter/test_enriched.parquet'\n",
      "Total time taken: 7.94 minutes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import uuid\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import json\n",
    "import matplotlib\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize_scalar\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "import traceback\n",
    "\n",
    "# Force Matplotlib to use a non-GUI backend\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- THE DEFINITIVE FIX for the \"ArrowDtype\" error ---\n",
    "if 'dask' in sys.modules:\n",
    "    del sys.modules['dask']\n",
    "\n",
    "def run_stage_0_advanced_feature_engineering():\n",
    "    print(\"--- Starting Advanced Feature Engineering Pipeline (v3) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- 1. Load All Data ---\n",
    "    print(\"1/7: Loading all raw data files...\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet('train_data.parquet')\n",
    "        test_df = pd.read_parquet('test_data_r3.parquet')\n",
    "        add_trans = pd.read_parquet('add_trans.parquet')\n",
    "        add_event = pd.read_parquet('add_event.parquet')\n",
    "        offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find a required data file. Make sure all .parquet files are in the same directory. Details: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Combine train and test for consistent processing\n",
    "    train_len = len(train_df)\n",
    "    all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # --- 2. Preprocessing & Timestamp Conversion ---\n",
    "    print(\"2/7: Processing timestamps...\")\n",
    "    all_data['id4'] = pd.to_datetime(all_data['id4'])\n",
    "    offer_meta['id12'] = pd.to_datetime(offer_meta['id12'])\n",
    "    offer_meta['id13'] = pd.to_datetime(offer_meta['id13'])\n",
    "    add_trans['f370'] = pd.to_datetime(add_trans['f370'])\n",
    "    add_event['id4'] = pd.to_datetime(add_event['id4'])\n",
    "    add_event['id7'] = pd.to_datetime(add_event['id7'], errors='coerce') # Click timestamp can be NaT\n",
    "\n",
    "    # --- 3. Engineer Static Offer Features (from offer_metadata) ---\n",
    "    print(\"3/7: Engineering static offer features (including NLP)...\")\n",
    "    offer_meta['offer_duration_days'] = (offer_meta['id13'] - offer_meta['id12']).dt.days\n",
    "    offer_meta['offer_start_dayofweek'] = offer_meta['id12'].dt.dayofweek\n",
    "    offer_meta['brand_id'], _ = pd.factorize(offer_meta['id11'])\n",
    "    offer_meta['industry_id'], _ = pd.factorize(offer_meta['id10'])\n",
    "    offer_meta.rename(columns={'f375': 'redemption_frequency', 'f376': 'discount_rate', 'id8': 'member_industry_code', 'id9': 'offer_name', 'f378': 'offer_body'}, inplace=True)\n",
    "    offer_meta['is_industry_match'] = (offer_meta['industry_id'] == offer_meta['member_industry_code']).astype(int)\n",
    "\n",
    "    # --- NEW v3 FEATURE: Basic NLP on Offer Text ---\n",
    "    print(\"  -> Generating NLP features from offer text...\")\n",
    "    offer_meta['offer_name'] = offer_meta['offer_name'].astype(str).str.lower()\n",
    "    offer_meta['offer_body'] = offer_meta['offer_body'].astype(str).str.lower()\n",
    "\n",
    "    offer_meta['offer_name_len'] = offer_meta['offer_name'].str.len()\n",
    "    offer_meta['offer_body_words'] = offer_meta['offer_body'].str.split().str.len()\n",
    "\n",
    "    # Keyword matching\n",
    "    offer_meta['has_keyword_cashback'] = offer_meta['offer_body'].str.contains('cash back|statement credit|refund', regex=True).astype(int)\n",
    "    offer_meta['has_keyword_points'] = offer_meta['offer_body'].str.contains('points|miles|rewards', regex=True).astype(int)\n",
    "    offer_meta['has_keyword_discount'] = offer_meta['offer_body'].str.contains('% off|discount|save', regex=True).astype(int)\n",
    "    offer_meta['has_keyword_spend_x'] = offer_meta['offer_body'].str.contains('spend', regex=False).astype(int)\n",
    "\n",
    "    # Select columns to merge\n",
    "    offer_features_to_merge = [\n",
    "        'id3', 'offer_duration_days', 'offer_start_dayofweek', 'brand_id', 'industry_id',\n",
    "        'redemption_frequency', 'discount_rate', 'is_industry_match',\n",
    "        'offer_name_len', 'offer_body_words', 'has_keyword_cashback', \n",
    "        'has_keyword_points', 'has_keyword_discount', 'has_keyword_spend_x',\n",
    "        'id12', 'id13'\n",
    "    ]\n",
    "    \n",
    "    # --- FIX for dtype mismatch error ---\n",
    "    # Ensure the merge key 'id3' is the same type in both dataframes before merging.\n",
    "    all_data['id3'] = all_data['id3'].astype('int64')\n",
    "    offer_meta['id3'] = offer_meta['id3'].astype('int64')\n",
    "\n",
    "    all_data = all_data.merge(offer_meta[offer_features_to_merge], on='id3', how='left')\n",
    "\n",
    "    # Create time-based interaction features\n",
    "    all_data['days_since_offer_start'] = (all_data['id4'] - all_data['id12']).dt.days\n",
    "    all_data['days_until_offer_end'] = (all_data['id13'] - all_data['id4']).dt.days\n",
    "    all_data.drop(columns=['id12', 'id13'], inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    # --- 4. Engineer Historical Customer-Offer Interaction Features (from add_event) ---\n",
    "    print(\"4/7: Engineering customer-offer historical interaction features (impressions, clicks & timing)...\")\n",
    "    \n",
    "    # --- FIX for dtype mismatch error ---\n",
    "    # Ensure the merge key 'id3' is the same type before merging.\n",
    "    add_event['id3'] = add_event['id3'].astype('int64')\n",
    "\n",
    "    add_event = add_event.merge(offer_meta[['id3', 'brand_id', 'industry_id']], on='id3', how='left')\n",
    "    add_event.sort_values(by=['id2', 'id4'], inplace=True)\n",
    "\n",
    "    # Create click features\n",
    "    add_event['clicked'] = add_event['id7'].notna().astype(int)\n",
    "\n",
    "    # --- NEW v3 FEATURE: Advanced Temporal Features ---\n",
    "    print(\"  -> Generating advanced temporal (time-since-last-event) features...\")\n",
    "    add_event['time_since_last_impression_seconds'] = add_event.groupby('id2')['id4'].diff().dt.total_seconds()\n",
    "    # Create a column with the timestamp only if a click occurred\n",
    "    add_event['click_timestamp'] = add_event['id4'].where(add_event['clicked'] == 1)\n",
    "    # Forward-fill the last click time for each customer\n",
    "    add_event['last_click_timestamp'] = add_event.groupby('id2')['click_timestamp'].ffill()\n",
    "    add_event['time_since_last_click_seconds'] = (add_event['id4'] - add_event['last_click_timestamp']).dt.total_seconds()\n",
    "\n",
    "    # Calculate lagged cumulative counts for impressions and clicks\n",
    "    # The shift(1) is critical to prevent data leakage from the current event\n",
    "    add_event['customer_total_impressions_before'] = add_event.groupby('id2').cumcount()\n",
    "    add_event['customer_brand_impressions_before'] = add_event.groupby(['id2', 'brand_id']).cumcount()\n",
    "    add_event['customer_total_clicks_before'] = add_event.groupby('id2')['clicked'].cumsum().shift(1).fillna(0)\n",
    "    add_event['customer_brand_clicks_before'] = add_event.groupby(['id2', 'brand_id'])['clicked'].cumsum().shift(1).fillna(0)\n",
    "\n",
    "    # Calculate historical Click-Through-Rates (CTR)\n",
    "    epsilon = 1e-6\n",
    "    add_event['customer_ctr_before'] = add_event['customer_total_clicks_before'] / (add_event['customer_total_impressions_before'] + epsilon)\n",
    "    add_event['customer_brand_ctr_before'] = add_event['customer_brand_clicks_before'] / (add_event['customer_brand_impressions_before'] + epsilon)\n",
    "\n",
    "    # Create a unique event key to merge these features back to the main dataframe\n",
    "    event_key = ['id2', 'id3', 'id4']\n",
    "    event_features_to_merge = [\n",
    "        'time_since_last_impression_seconds', 'time_since_last_click_seconds',\n",
    "        'customer_total_impressions_before', 'customer_brand_impressions_before',\n",
    "        'customer_total_clicks_before', 'customer_brand_clicks_before',\n",
    "        'customer_ctr_before', 'customer_brand_ctr_before'\n",
    "    ]\n",
    "    \n",
    "    # --- FIX for composite key dtype mismatch ---\n",
    "    # Ensure all columns in the merge key have the same dtype before merging.\n",
    "    for col in event_key:\n",
    "        all_data[col] = all_data[col].astype(add_event[col].dtype)\n",
    "\n",
    "    all_data = all_data.merge(add_event[event_key + event_features_to_merge], on=event_key, how='left')\n",
    "    del add_event\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # --- 5. Engineer Customer Transaction Features (from add_trans) --- LEAKAGE FIXED ---\n",
    "    print(\"5/7: Engineering POINT-IN-TIME customer transaction features to prevent data leakage...\")\n",
    "    \n",
    "    # Ensure add_trans has a proper timestamp and is sorted for time-based operations\n",
    "    # --- FIX for ValueError on timedelta conversion ---\n",
    "    # Force f371 to numeric, coercing any non-numeric values (like strings) to NaT.\n",
    "    # This is a robust way to handle potentially dirty data in the time column.\n",
    "    time_as_seconds = pd.to_numeric(add_trans['f371'], errors='coerce')\n",
    "    add_trans['transaction_timestamp'] = add_trans['f370'] + pd.to_timedelta(time_as_seconds, unit='s')\n",
    "    \n",
    "    add_trans.sort_values(by=['id2', 'transaction_timestamp'], inplace=True)\n",
    "\n",
    "    # --- Create a unified transaction history dataframe ---\n",
    "    # Calculate cumulative features directly on the sorted transaction data\n",
    "    print(\"  -> Calculating point-in-time cumulative aggregates...\")\n",
    "    \n",
    "    # Use a temporary dataframe to build aggregates\n",
    "    trans_history = add_trans[['id2', 'transaction_timestamp', 'f367', 'f368', 'f369']].copy()\n",
    "    \n",
    "    # Debits and Credits\n",
    "    is_debit = trans_history['f369'] == 'D'\n",
    "    is_credit = trans_history['f369'] == 'C'\n",
    "    debit_amount = trans_history['f367'].where(is_debit, 0)\n",
    "    credit_amount = trans_history['f367'].where(is_credit, 0)\n",
    "\n",
    "    # Cumulative calculations\n",
    "    grouped = trans_history.groupby('id2')\n",
    "    trans_history['customer_total_spend'] = grouped['f367'].transform(lambda x: x[is_debit].cumsum())\n",
    "    trans_history['customer_num_transactions'] = grouped.cumcount() + 1\n",
    "    \n",
    "    trans_history['customer_num_debits'] = grouped['f369'].transform(lambda x: (x == 'D').cumsum())\n",
    "    trans_history['customer_num_refunds'] = grouped['f369'].transform(lambda x: (x == 'C').cumsum())\n",
    "    \n",
    "    trans_history['customer_total_refund_amount'] = grouped['f367'].transform(lambda x: x[is_credit].cumsum())\n",
    "\n",
    "    # Fill NaNs that result from cumsum on filtered series\n",
    "    for col in ['customer_total_spend', 'customer_total_refund_amount', 'customer_num_debits', 'customer_num_refunds']:\n",
    "        trans_history[col] = grouped[col].ffill().fillna(0)\n",
    "\n",
    "    # Point-in-time ratios and averages\n",
    "    epsilon = 1e-6\n",
    "    trans_history['customer_avg_trans_amount'] = trans_history['customer_total_spend'] / (trans_history['customer_num_debits'] + epsilon)\n",
    "    trans_history['customer_refund_rate'] = trans_history['customer_num_refunds'] / (trans_history['customer_num_debits'] + epsilon)\n",
    "    \n",
    "    # --- FIX for DataError: No numeric types to aggregate ---\n",
    "    # The default .expanding() tries to cast to numeric. For object types like Product ID,\n",
    "    # we need a method that is safe for strings. This custom apply is robust and avoids\n",
    "    # the internal numeric casting of the expanding window function.\n",
    "    def expanding_nunique_for_objects(series):\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for x in series:\n",
    "            seen.add(x)\n",
    "            result.append(len(seen))\n",
    "        return pd.Series(result, index=series.index)\n",
    "    \n",
    "    trans_history['customer_num_unique_products'] = grouped['f368'].transform(expanding_nunique_for_objects)\n",
    "\n",
    "    # --- Merge point-in-time features back to the main dataset ---\n",
    "    print(\"  -> Merging point-in-time transaction features into main dataset...\")\n",
    "    \n",
    "    # --- FIX for ValueError: left keys must be sorted ---\n",
    "    # merge_asof requires the 'on' key (the timestamp) to be sorted in the left dataframe.\n",
    "    all_data.sort_values(by='id4', inplace=True)\n",
    "\n",
    "    features_to_merge = [\n",
    "        'id2', 'transaction_timestamp', 'customer_total_spend', 'customer_num_transactions',\n",
    "        'customer_avg_trans_amount', 'customer_num_unique_products', 'customer_total_refund_amount',\n",
    "        'customer_num_refunds', 'customer_refund_rate'\n",
    "    ]\n",
    "    \n",
    "    # --- FIX for ValueError: Merge keys contain nulls ---\n",
    "    # merge_asof cannot handle nulls in the 'on' or 'by' keys of the right dataframe.\n",
    "    # We drop any rows from our history where the timestamp is invalid.\n",
    "    trans_history.dropna(subset=['id2', 'transaction_timestamp'], inplace=True)\n",
    "    \n",
    "    # Use merge_asof to get the LATEST transaction state for each event\n",
    "    all_data = pd.merge_asof(\n",
    "        all_data,\n",
    "        trans_history[features_to_merge],\n",
    "        left_on='id4',\n",
    "        right_on='transaction_timestamp',\n",
    "        by='id2',\n",
    "        direction='backward' # This is crucial for point-in-time correctness\n",
    "    )\n",
    "    \n",
    "    # Also calculate the days since the last transaction\n",
    "    all_data['days_since_last_transaction'] = (all_data['id4'] - all_data['transaction_timestamp']).dt.days\n",
    "    all_data.drop(columns=['transaction_timestamp'], inplace=True)\n",
    "\n",
    "    del add_trans, trans_history\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # --- 6. Finalize and Save ---\n",
    "    print(\"6/7: Finalizing datasets and saving to disk...\")\n",
    "    # Split back into train and test\n",
    "    train_enriched = all_data.iloc[:train_len].copy()\n",
    "    test_enriched = all_data.iloc[train_len:].copy()\n",
    "\n",
    "    # A final check for any NaNs introduced by merges, filling with a neutral value like -1 or 0.\n",
    "    print(\"  -> Filling NaN values introduced during merges...\")\n",
    "    new_numeric_cols = [\n",
    "        'offer_duration_days', 'offer_start_dayofweek', 'brand_id', 'industry_id',\n",
    "        'redemption_frequency', 'discount_rate', 'is_industry_match', 'days_since_offer_start', \n",
    "        'days_until_offer_end', 'offer_name_len', 'offer_body_words', 'has_keyword_cashback',\n",
    "        'has_keyword_points', 'has_keyword_discount', 'has_keyword_spend_x',\n",
    "        'time_since_last_impression_seconds', 'time_since_last_click_seconds',\n",
    "        'customer_total_impressions_before', 'customer_brand_impressions_before',\n",
    "        'customer_total_clicks_before', 'customer_brand_clicks_before', 'customer_ctr_before', \n",
    "        'customer_brand_ctr_before', 'customer_avg_trans_amount', 'customer_total_spend',\n",
    "        'customer_num_transactions', 'customer_num_unique_products', 'days_since_last_transaction',\n",
    "        'customer_total_refund_amount', 'customer_num_refunds', 'customer_refund_rate'\n",
    "    ]\n",
    "    for col in new_numeric_cols:\n",
    "        if col in train_enriched.columns:\n",
    "            # For time-based features, a large negative number might be better than 0 or -1\n",
    "            if 'seconds' in col or 'days' in col:\n",
    "                train_enriched[col] = train_enriched[col].fillna(-999)\n",
    "                test_enriched[col] = test_enriched[col].fillna(-999)\n",
    "            # For rates and counts, 0 is a more natural fill value\n",
    "            elif 'rate' in col or 'num_' in col or 'total_' in col or 'avg' in col or 'before' in col:\n",
    "                train_enriched[col] = train_enriched[col].fillna(0)\n",
    "                test_enriched[col] = test_enriched[col].fillna(0)\n",
    "            else: # For IDs and other general features\n",
    "                train_enriched[col] = train_enriched[col].fillna(-1)\n",
    "                test_enriched[col] = test_enriched[col].fillna(-1)\n",
    "\n",
    "\n",
    "    # --- 7. Save Final Datasets ---\n",
    "    os.makedirs('inter', exist_ok=True)\n",
    "    train_enriched.to_parquet('inter/train_enriched.parquet', index=False)\n",
    "    test_enriched.to_parquet('inter/test_enriched.parquet', index=False)\n",
    "    del all_data, train_enriched, test_enriched\n",
    "    gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Advanced Feature Engineering (v3) Complete! ---\")\n",
    "    print(f\"✅ Saved 'inter/train_enriched.parquet'\")\n",
    "    print(f\"✅ Saved 'inter/test_enriched.parquet'\")\n",
    "    print(f\"Total time taken: {((end_time - start_time) / 60):.2f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Stage 0: Create rich features from raw data\n",
    "    run_stage_0_advanced_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6602808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Script 0: Train-Val Split (Customer-Aware & Time-Based)**\n",
      "1/5: Loading IDs and timestamps to determine validation customers...\n",
      "1/5 ID index DataFrame created with shape: (770164, 2)\n",
      "2/5: Identified 7991 customers for the time-based validation set.\n",
      "3/5: Splitting full dataset based on validation customer IDs...\n",
      "3/5 Saved train_0.parquet with shape: (589815, 403)\n",
      "4/5 Saved valid_0.parquet with shape: (180349, 403)\n",
      "5/5 Saved test_0.parquet with shape: (337714, 403)\n"
     ]
    }
   ],
   "source": [
    "def run_stage_1_train_val_split():\n",
    "    TRAIN_INPUT = \"inter/train_enriched.parquet\"\n",
    "    TEST_INPUT = \"inter/test_enriched.parquet\"\n",
    "    INTER_DIR = \"inter\"\n",
    "    BATCH_SIZE = 100_000\n",
    "    VAL_RATIO = 0.15\n",
    "\n",
    "    print(\"\\n**Script 0: Train-Val Split (Customer-Aware & Time-Based)**\")\n",
    "    os.makedirs(INTER_DIR, exist_ok=True)\n",
    "\n",
    "    parquet_file = pq.ParquetFile(TRAIN_INPUT)\n",
    "\n",
    "    # 1. Load IDs and timestamps to determine the split\n",
    "    print(\"1/5: Loading IDs and timestamps to determine validation customers...\")\n",
    "    id_chunks = []\n",
    "    # CRITICAL FIX: Load customer ID 'id2' to ensure customer-based split\n",
    "    for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE, columns=[\"id2\", \"id4\"]):\n",
    "        chunk_df = batch.to_pandas()\n",
    "        id_chunks.append(chunk_df)\n",
    "\n",
    "    id_df = pd.concat(id_chunks, ignore_index=True)\n",
    "    print(f\"1/5 ID index DataFrame created with shape: {id_df.shape}\")\n",
    "    del id_chunks\n",
    "    gc.collect()\n",
    "\n",
    "    id_df[\"id4\"] = pd.to_datetime(id_df[\"id4\"])\n",
    "    id_df.sort_values(by=\"id4\", inplace=True)\n",
    "\n",
    "    # 2. Identify validation customers based on time\n",
    "    # This ensures that the validation set is from a later time period than the training set.\n",
    "    n_rows = len(id_df)\n",
    "    split_idx = int(n_rows * (1 - VAL_RATIO))\n",
    "    valid_customer_ids = set(id_df[\"id2\"].iloc[split_idx:])\n",
    "    print(f\"2/5: Identified {len(valid_customer_ids)} customers for the time-based validation set.\")\n",
    "    del id_df\n",
    "    gc.collect()\n",
    "\n",
    "    # 3. Stream data and split into train/valid based on the identified customer IDs\n",
    "    print(\"3/5: Splitting full dataset based on validation customer IDs...\")\n",
    "    train_rows = []\n",
    "    valid_rows = []\n",
    "\n",
    "    for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):\n",
    "        batch_df = batch.to_pandas()\n",
    "        is_valid_customer = batch_df[\"id2\"].isin(valid_customer_ids)\n",
    "        valid_rows.append(batch_df[is_valid_customer])\n",
    "        train_rows.append(batch_df[~is_valid_customer])\n",
    "    del parquet_file, valid_customer_ids\n",
    "\n",
    "    # 4. Save the split datasets\n",
    "    train_split = pd.concat(train_rows, ignore_index=True)\n",
    "    train_split.to_parquet(f\"{INTER_DIR}/train_0.parquet\", index=False)\n",
    "    print(f\"3/5 Saved train_0.parquet with shape: {train_split.shape}\")\n",
    "    del train_split, train_rows\n",
    "    gc.collect()\n",
    "\n",
    "    valid_split = pd.concat(valid_rows, ignore_index=True)\n",
    "    valid_split.to_parquet(f\"{INTER_DIR}/valid_0.parquet\", index=False)\n",
    "    print(f\"4/5 Saved valid_0.parquet with shape: {valid_split.shape}\")\n",
    "    del valid_split, valid_rows\n",
    "    gc.collect()\n",
    "\n",
    "    # 5. Copy test data for consistency\n",
    "    test_df = pd.read_parquet(TEST_INPUT)\n",
    "    test_df.to_parquet(f\"{INTER_DIR}/test_0.parquet\", index=False)\n",
    "    print(f\"5/5 Saved test_0.parquet with shape: {test_df.shape}\")\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Stage 1: Split enriched training data into train/validation sets\n",
    "    run_stage_1_train_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d69796-0dce-4ab0-bdf5-2e8f13806c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Script 1: Unified Cleaning and Preprocessing\n",
      "\n",
      "\n",
      "--- Step 1: Removing 100% Empty Features ---\n",
      "Found 4 empty columns to drop.\n",
      "\n",
      "--- Step 2: Applying Type Conversions and Encoding ---\n",
      "Numerical, OHE, and Datetime conversions complete.\n",
      "Applying type conversion to 29 newly engineered numeric features...\n",
      "Applying Label Encoding to 13 categorical features...\n",
      "All preprocessing is complete.\n",
      "All files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(train_df, valid_df, test_df, data_dict_path):\n",
    "    \"\"\"\n",
    "    A comprehensive function to clean and typecast datasets.\n",
    "    1. Removes 100% empty columns based on the training set.\n",
    "    2. Applies robust type casting for numerical and OHE features.\n",
    "    3. Applies consistent Label Encoding for all categorical features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Removing 100% Empty Features ---\")\n",
    "\n",
    "    # Identify columns that are completely null in the training data\n",
    "    uniques_per_col = train_df.nunique(dropna=True)\n",
    "    empty_cols = uniques_per_col[uniques_per_col == 0].index.tolist()\n",
    "\n",
    "    if empty_cols:\n",
    "        print(f\"Found {len(empty_cols)} empty columns to drop.\")\n",
    "        # Drop from all datasets for consistency\n",
    "        train_df.drop(columns=empty_cols, inplace=True)\n",
    "        valid_df.drop(columns=empty_cols, inplace=True)\n",
    "        test_df.drop(columns=empty_cols, inplace=True)\n",
    "    else:\n",
    "        print(\"No completely empty columns found in the training set.\")\n",
    "\n",
    "    print(\"\\n--- Step 2: Applying Type Conversions and Encoding ---\")\n",
    "    data_dict = pd.read_csv(data_dict_path)\n",
    "    type_map = data_dict.set_index(\"masked_column\")[\"Type\"].str.strip().to_dict()\n",
    "\n",
    "    # Identify all feature types from the dictionary\n",
    "    all_cols = train_df.columns\n",
    "    numerical_cols = [\n",
    "        col\n",
    "        for col, dtype in type_map.items()\n",
    "        if dtype == \"Numerical\" and col in all_cols\n",
    "    ]\n",
    "    numerical_cols.append(\"f218\")\n",
    "\n",
    "    ohe_cols = [\n",
    "        col\n",
    "        for col, dtype in type_map.items()\n",
    "        if dtype == \"One hot encoded\" and col in all_cols\n",
    "    ]\n",
    "\n",
    "    categorical_cols = [\n",
    "        col\n",
    "        for col, dtype in type_map.items()\n",
    "        if dtype == \"Categorical\" and col in all_cols\n",
    "    ]\n",
    "    if \"id3\" in categorical_cols:\n",
    "        categorical_cols.remove(\"id3\")\n",
    "\n",
    "    datetime_cols = [\"id4\", \"id5\"]\n",
    "    label_col = \"y\"\n",
    "\n",
    "    # --- FIX: Explicitly add new categorical features for proper encoding ---\n",
    "    new_categorical_features = ['brand_id', 'industry_id']\n",
    "    for new_cat in new_categorical_features:\n",
    "        if new_cat not in categorical_cols:\n",
    "            categorical_cols.append(new_cat)\n",
    "\n",
    "    # Process all three dataframes\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        # Numerical conversion\n",
    "        for col in numerical_cols:\n",
    "            if col not in datetime_cols and col in df.columns:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        # One-Hot Encoded conversion\n",
    "        for col in ohe_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(np.float32).fillna(0).astype(np.int8)\n",
    "\n",
    "        # Datetime conversion\n",
    "        if \"id4\" in df.columns:\n",
    "            df[\"id4\"] = pd.to_datetime(df[\"id4\"])\n",
    "        if \"id5\" in df.columns:\n",
    "            df[\"id5\"] = pd.to_datetime(df[\"id5\"])\n",
    "\n",
    "    print(\"Numerical, OHE, and Datetime conversions complete.\")\n",
    "\n",
    "    # --- FIX: Explicitly handle new numeric features for memory efficiency ---\n",
    "    new_numeric_cols = [\n",
    "        'offer_duration_days', 'offer_start_dayofweek', 'redemption_frequency', 'discount_rate', 'is_industry_match', 'days_since_offer_start',\n",
    "        'days_until_offer_end', 'offer_name_len', 'offer_body_words', 'has_keyword_cashback',\n",
    "        'has_keyword_points', 'has_keyword_discount', 'has_keyword_spend_x',\n",
    "        'time_since_last_impression_seconds', 'time_since_last_click_seconds',\n",
    "        'customer_total_impressions_before', 'customer_brand_impressions_before',\n",
    "        'customer_total_clicks_before', 'customer_brand_clicks_before', 'customer_ctr_before',\n",
    "        'customer_brand_ctr_before', 'customer_avg_trans_amount', 'customer_total_spend',\n",
    "        'customer_num_transactions', 'customer_num_unique_products', 'days_since_last_transaction',\n",
    "        'customer_total_refund_amount', 'customer_num_refunds', 'customer_refund_rate'\n",
    "    ]\n",
    "    print(f\"Applying type conversion to {len(new_numeric_cols)} newly engineered numeric features...\")\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        for col in new_numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    print(f\"Applying Label Encoding to {len(categorical_cols)} categorical features...\")\n",
    "    for col in categorical_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df[col].fillna(\"Missing\")\n",
    "            valid_df[col] = valid_df[col].fillna(\"Missing\")\n",
    "            test_df[col] = test_df[col].fillna(\"Missing\")\n",
    "\n",
    "            learned_categories = train_df[col].astype(\"category\").dtype\n",
    "\n",
    "            train_df[col] = train_df[col].astype(learned_categories)\n",
    "            valid_df[col] = valid_df[col].astype(learned_categories)\n",
    "            test_df[col] = test_df[col].astype(learned_categories)\n",
    "\n",
    "            train_df[col] = train_df[col].cat.codes\n",
    "            valid_df[col] = valid_df[col].cat.codes\n",
    "            test_df[col] = test_df[col].cat.codes\n",
    "\n",
    "    for df in [train_df, valid_df]:\n",
    "        if label_col in df.columns:\n",
    "            df[label_col] = df[label_col].astype(np.int8)\n",
    "\n",
    "    print(\"All preprocessing is complete.\")\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "def run_stage_2_preprocessing():\n",
    "    DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "    INTER_DIR = \"inter\"\n",
    "    print(\"\\n**Script 1: Unified Cleaning and Preprocessing\\n\")\n",
    "\n",
    "    train_df = pd.read_parquet(f\"{INTER_DIR}/train_0.parquet\")\n",
    "    test_df = pd.read_parquet(f\"{INTER_DIR}/test_0.parquet\")\n",
    "    valid_df = pd.read_parquet(f\"{INTER_DIR}/valid_0.parquet\")\n",
    "\n",
    "    processed_train, processed_valid, processed_test = preprocess_data(\n",
    "        train_df, valid_df, test_df, DATA_DICT_PATH\n",
    "    )\n",
    "\n",
    "    processed_train.to_parquet(f\"{INTER_DIR}/train_1.parquet\")\n",
    "    processed_test.to_parquet(f\"{INTER_DIR}/test_1.parquet\")\n",
    "    processed_valid.to_parquet(f\"{INTER_DIR}/valid_1.parquet\")\n",
    "    print(\"All files saved successfully.\")\n",
    "\n",
    "    del train_df, test_df, valid_df, processed_train, processed_valid, processed_test\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Stage 2: Clean and preprocess the split datasets\n",
    "    run_stage_2_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e939e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ***Script 2: Final Production-Grade Parallel Feature Engineering (Leakage Fixed) ***\n",
      "\n",
      "--- Processing Training Data ---\n",
      "\n",
      "Processing DataFrame with shape: (589815, 399) to create file: inter/train_2_final.parquet\n",
      "Using 30 cores.\n",
      "  -> Created temporary directory: inter/temp_splits\n",
      "  -> Created temporary directory: inter/temp_parts\n",
      "  -> Created temporary directory: inter/temp_combined\n",
      "Step 1/5: Engineering fast, vectorized features...\n",
      "Step 2/5: Computing offer profiles...\n",
      "Step 2.5/5: Engineering leakage-free historical features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Mapping offer profiles: 100%|██████████| 4/4 [00:00<00:00, 100.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.7/5: Engineering dynamic customer profiles for all features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Calculating dynamic profiles: 100%|██████████| 267/267 [09:31<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3/5: Pre-splitting data for 38559 customers in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Parallel Splitting Progress: 100%|██████████| 120/120 [45:43<00:00, 22.86s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.5/5: Discovering customer files from disk to process...\n",
      "   -> Found 38559 customer files to process.\n",
      "Processing all 38559 customer files in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Feature Engineering Progress: 100%|██████████| 38559/38559 [1:11:23<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4/5: Combining processed parts from disk...\n",
      "  -> No master schema found. Creating a robust union schema from a sample of part files...\n",
      "  -> Master schema created successfully.\n",
      "Step 4.5/5: Combining 38559 parts in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Combining Chunks Progress: 100%|██████████| 120/120 [09:56<00:00,  4.97s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5/5: Writing final output file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Writing Final File: 100%|██████████| 120/120 [02:57<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating file: inter/train_2_final.parquet\n",
      "\n",
      "--- Processing Validation Data ---\n",
      "\n",
      "Processing DataFrame with shape: (180349, 399) to create file: inter/valid_2_final.parquet\n",
      "Using 30 cores.\n",
      "  -> Created temporary directory: inter/temp_splits\n",
      "  -> Created temporary directory: inter/temp_parts\n",
      "  -> Created temporary directory: inter/temp_combined\n",
      "Step 1/5: Engineering fast, vectorized features...\n",
      "Step 2.5/5: Engineering leakage-free historical features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Mapping offer profiles: 100%|██████████| 4/4 [00:00<00:00, 273.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.7/5: Engineering dynamic customer profiles for all features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Calculating dynamic profiles: 100%|██████████| 267/267 [02:07<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3/5: Pre-splitting data for 7991 customers in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Parallel Splitting Progress: 100%|██████████| 120/120 [12:42<00:00,  6.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.5/5: Discovering customer files from disk to process...\n",
      "   -> Found 7991 customer files to process.\n",
      "Processing all 7991 customer files in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Feature Engineering Progress: 100%|██████████| 7991/7991 [14:45<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4/5: Combining processed parts from disk...\n",
      "Step 4.5/5: Combining 7991 parts in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Combining Chunks Progress: 100%|██████████| 120/120 [01:48<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5/5: Writing final output file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Writing Final File: 100%|██████████| 120/120 [02:04<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating file: inter/valid_2_final.parquet\n",
      "\n",
      "--- Processing Test Data ---\n",
      "\n",
      "Processing DataFrame with shape: (337714, 399) to create file: inter/test_2_final.parquet\n",
      "Using 30 cores.\n",
      "  -> Created temporary directory: inter/temp_splits\n",
      "  -> Created temporary directory: inter/temp_parts\n",
      "  -> Created temporary directory: inter/temp_combined\n",
      "Step 1/5: Engineering fast, vectorized features...\n",
      "Step 2.5/5: Engineering leakage-free historical features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Mapping offer profiles: 100%|██████████| 4/4 [00:00<00:00, 158.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.7/5: Engineering dynamic customer profiles for all features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Calculating dynamic profiles: 100%|██████████| 267/267 [04:45<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3/5: Pre-splitting data for 18956 customers in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Parallel Splitting Progress: 100%|██████████| 120/120 [26:02<00:00, 13.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3.5/5: Discovering customer files from disk to process...\n",
      "   -> Found 18956 customer files to process.\n",
      "Processing all 18956 customer files in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Feature Engineering Progress: 100%|██████████| 18956/18956 [35:12<00:00,  8.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4/5: Combining processed parts from disk...\n",
      "Step 4.5/5: Combining 18956 parts in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Combining Chunks Progress: 100%|██████████| 120/120 [04:20<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5/5: Writing final output file...\n",
      " -> Test run detected. Removing 'y' column from final output schema.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -> Writing Final File: 100%|██████████| 120/120 [02:25<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating file: inter/test_2_final.parquet\n",
      "\n",
      "--- All datasets have been feature-engineered and saved successfully. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import uuid\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import json\n",
    "import matplotlib\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize_scalar\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "\n",
    "# Force Matplotlib to use a non-GUI backend\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "SKIP_TRAINING = False\n",
    "DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "INPUT_DIR = \"inter\"\n",
    "OUTPUT_DIR = \"inter\"\n",
    "TEMP_SPLIT_DIR = os.path.join(OUTPUT_DIR, \"temp_splits\")\n",
    "TEMP_PARTS_DIR = os.path.join(OUTPUT_DIR, \"temp_parts\")\n",
    "TEMP_COMBINED_DIR = os.path.join(OUTPUT_DIR, \"temp_combined\")\n",
    "N_CORES = cpu_count()\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def downcast_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcasts float64 columns to the more memory-efficient float32 type.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "# --- Worker for Phase 1 - Parallel Splitting ---\n",
    "def split_customer_data_worker(customer_id_chunk, main_df, output_dir):\n",
    "    \"\"\"\n",
    "    Takes a chunk of customer IDs, filters the main DataFrame for those customers,\n",
    "    and saves each customer's data to a separate file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sub_df = main_df[main_df['id2'].isin(customer_id_chunk)]\n",
    "        for customer_id, group_df in sub_df.groupby('id2'):\n",
    "            output_file = os.path.join(output_dir, f\"customer_{customer_id}.parquet\")\n",
    "            group_df.to_parquet(output_file)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in splitting worker for chunk starting with {customer_id_chunk[0]}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Worker for Phase 2 - Parallel Feature Engineering ---\n",
    "def process_customer_file(customer_id, input_dir, output_dir, numerical_cols):\n",
    "    \"\"\"\n",
    "    Processes a SINGLE customer's data by reading it from a pre-split file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input_path = os.path.join(input_dir, f\"customer_{customer_id}.parquet\")\n",
    "        group_df = pd.read_parquet(input_path)\n",
    "        \n",
    "        # --- DEFINITIVE FIX V3: Robust Cleaning & Defensive Programming ---\n",
    "        # 1. Defensively clean any object columns that might contain lists or mixed types.\n",
    "        def clean_mixed_type(x):\n",
    "            if isinstance(x, (list, tuple, np.ndarray)):\n",
    "                return x[0] if len(x) > 0 else np.nan\n",
    "            return x\n",
    "\n",
    "        for col in group_df.columns:\n",
    "            if group_df[col].dtype == 'object':\n",
    "                # This .apply is only used on object columns to avoid performance hits\n",
    "                group_df[col] = group_df[col].apply(clean_mixed_type)\n",
    "        \n",
    "        # 2. Force all feature columns to numeric, coercing any post-cleaning errors.\n",
    "        f_cols = [c for c in group_df.columns if c.startswith('f')]\n",
    "        if f_cols:\n",
    "            group_df[f_cols] = group_df[f_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # 3. Create a clean copy to work with, preventing SettingWithCopyWarning\n",
    "        group_df = group_df.copy()\n",
    "\n",
    "        # The previous, less effective cleaning blocks are now removed.\n",
    "\n",
    "        # --- Helper functions ---\n",
    "        def wma(series, period):\n",
    "            # The series is now guaranteed to be numeric, so direct operations are safe.\n",
    "            return series.ewm(alpha=2 / (period + 1), adjust=False).mean()\n",
    "        \n",
    "        def hma(series, period):\n",
    "            if period <= 1: return series\n",
    "            period_sqrt = int(np.sqrt(period)); period_sqrt = 1 if period_sqrt < 1 else period_sqrt\n",
    "            wma1 = 2 * wma(series, period // 2); wma2 = wma(series, period)\n",
    "            return wma(wma1 - wma2, period_sqrt)\n",
    "        \n",
    "        def safe_shannon_entropy(series):\n",
    "            if len(series) < 15: return np.nan\n",
    "            \n",
    "            # The series is now numeric, but may have NaNs from rolling windows.\n",
    "            # Interpolate and then fill any remaining NaNs at the edges\n",
    "            interpolated_series = series.interpolate(method='linear').ffill().bfill()\n",
    "            \n",
    "            # Final check for validity before FFT\n",
    "            if interpolated_series.isnull().any() or interpolated_series.nunique() < 2: return np.nan\n",
    "            \n",
    "            psd = np.abs(rfft(interpolated_series.values))**2\n",
    "            if psd.sum() == 0: return np.nan\n",
    "            psd_prob = psd / psd.sum()\n",
    "            return scipy.stats.entropy(psd_prob)\n",
    "\n",
    "        # --- Prerequisite variables with additional safety ---\n",
    "        # Ensure id4 is properly datetime before calculations\n",
    "        if group_df['id4'].dtype != 'datetime64[ns]':\n",
    "            group_df['id4'] = pd.to_datetime(group_df['id4'], errors='coerce')\n",
    "        \n",
    "        days_since_last_event = (group_df['id4'].diff().dt.total_seconds() / (3600 * 24)) + 1e-6\n",
    "        # Additional safety: ensure days_since_last_event is numeric\n",
    "        days_since_last_event = pd.to_numeric(days_since_last_event, errors='coerce').fillna(1e-6)\n",
    "        \n",
    "        is_click = (group_df['f29'].diff() > 0) if 'f29' in group_df.columns else pd.Series([False] * len(group_df), index=group_df.index)\n",
    "        group_df['is_click_int'] = is_click.astype(int)\n",
    "\n",
    "        # --- Feature Engineering Blocks ---\n",
    "        group_df['offer_interaction_count'] = group_df.groupby('id3').cumcount()\n",
    "        if 'offer_category' in group_df.columns:\n",
    "            group_df['category_interaction_count'] = group_df.groupby('offer_category').cumcount()\n",
    "            category_clicks = group_df.groupby('offer_category')['is_click_int'].cumsum().shift(1).fillna(0)\n",
    "            category_views = group_df.groupby('offer_category').cumcount()\n",
    "            group_df['customer_category_ctr'] = category_clicks / (category_views + 1)\n",
    "        if 'session_id' in group_df.columns:\n",
    "            session_gb = group_df.groupby('session_id')\n",
    "            group_df['session_offer_count'] = session_gb.cumcount()\n",
    "            group_df['session_clicks'] = session_gb['is_click_int'].cumsum().shift(1).fillna(0)\n",
    "            group_df['time_since_session_start_mins'] = (group_df['id4'] - session_gb['id4'].transform('min')).dt.total_seconds() / 60\n",
    "        \n",
    "        if 'offer_age_days' in group_df.columns:\n",
    "            rolling_avg_age = group_df['offer_age_days'].rolling(10, min_periods=1).mean()\n",
    "            group_df['age_vs_recent_avg'] = group_df['offer_age_days'] - rolling_avg_age\n",
    "        group_df['time_since_last_event_hours'] = days_since_last_event * 24\n",
    "        if 'f29' in group_df.columns:\n",
    "            click_timestamps = group_df['id4'].where(is_click).ffill()\n",
    "            group_df['time_since_last_click'] = (group_df['id4'] - click_timestamps).dt.total_seconds() / 3600\n",
    "        \n",
    "        # === DEFINITIVE FIX 1/3: Add data cleaning to Velocity loop ===\n",
    "        key_velocity_cols = {'f43': 'balance', 'f77': 'engagement_ratio', 'f59': 'time_spent'}\n",
    "        for col, name in key_velocity_cols.items():\n",
    "            if col in group_df.columns:\n",
    "                # The robust cleaning at the start of the function ensures s_numeric is valid\n",
    "                s_numeric = group_df[col]\n",
    "                if s_numeric.isnull().all():\n",
    "                    group_df[f'{name}_velocity'] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                change = s_numeric.diff()\n",
    "                velocity = change / days_since_last_event\n",
    "                \n",
    "                # --- CORRECTED CODE (per user recommendation) ---\n",
    "                # Compute 1st and 99th percentiles separately (as scalars, not as a list)\n",
    "                lower_q = velocity.expanding(min_periods=5).quantile(0.01).ffill()\n",
    "                upper_q = velocity.expanding(min_periods=5).quantile(0.99).ffill()\n",
    "\n",
    "                # Clip velocity values to the [lower_q, upper_q] range.\n",
    "                # .clip() correctly handles NaN bounds by not clipping those values.\n",
    "                group_df[f'{name}_velocity'] = velocity.clip(lower_q, upper_q)\n",
    "\n",
    "        # === DEFINITIVE FIX 2/3: Add data cleaning to Rolling Features loop ===\n",
    "        for col in numerical_cols:\n",
    "            if col in group_df.columns:\n",
    "                # EXTRA SAFETY: Double-check for list-like values before any operations\n",
    "                if group_df[col].dtype == 'object':\n",
    "                    group_df[col] = group_df[col].apply(\n",
    "                        lambda x: x[0] if isinstance(x, (list, tuple, np.ndarray)) and len(x) > 0 else (np.nan if isinstance(x, (list, tuple, np.ndarray)) else x)\n",
    "                    )\n",
    "                \n",
    "                s_numeric = pd.to_numeric(group_df[col], errors='coerce')\n",
    "                if s_numeric.isnull().all():\n",
    "                    group_df[f'{col}_diff1'] = np.nan\n",
    "                    for k in [5, 10, 20]:\n",
    "                        group_df[f'{col}hma{k}'] = np.nan\n",
    "                    for k in [10, 20]:\n",
    "                        group_df[f'{col}roll_std{k}'] = np.nan\n",
    "                    continue\n",
    "\n",
    "                group_df[f'{col}_diff1'] = s_numeric.diff(1)\n",
    "                for k in [5, 10, 20]:\n",
    "                    group_df[f'{col}hma{k}'] = hma(s_numeric, k)\n",
    "                for k in [10, 20]:\n",
    "                    group_df[f'{col}roll_std{k}'] = s_numeric.rolling(k, min_periods=3).std()\n",
    "\n",
    "        # === DEFINITIVE FIX 3/3: Add data cleaning to Time-Series Features loop ===\n",
    "        key_ts_cols = ['f218', 'f219', 'f220', 'f102']\n",
    "        for col in key_ts_cols:\n",
    "            if col in group_df.columns:\n",
    "                # EXTRA SAFETY: Double-check for list-like values before any operations\n",
    "                if group_df[col].dtype == 'object':\n",
    "                    group_df[col] = group_df[col].apply(\n",
    "                        lambda x: x[0] if isinstance(x, (list, tuple, np.ndarray)) and len(x) > 0 else (np.nan if isinstance(x, (list, tuple, np.ndarray)) else x)\n",
    "                    )\n",
    "                \n",
    "                s_numeric = pd.to_numeric(group_df[col], errors='coerce')\n",
    "                if s_numeric.isnull().all():\n",
    "                    group_df[f'{col}_stability_5'] = np.nan\n",
    "                    group_df[f'{col}_lumpiness_5'] = np.nan\n",
    "                    group_df[f'{col}_rolling_shannon_entropy'] = np.nan\n",
    "                    continue\n",
    "\n",
    "                rm = s_numeric.rolling(5, min_periods=1).mean()\n",
    "                rv = s_numeric.rolling(5, min_periods=1).var()\n",
    "                group_df[f'{col}_stability_5'] = rm.rolling(5, min_periods=1).var()\n",
    "                group_df[f'{col}_lumpiness_5'] = rv.rolling(5, min_periods=1).var()\n",
    "                group_df[f'{col}_rolling_shannon_entropy'] = s_numeric.rolling(20, min_periods=15).apply(safe_shannon_entropy, raw=False)\n",
    "\n",
    "        group_df['time_since_last_seen_this_offer'] = group_df.groupby('id3')['id4'].diff().dt.total_seconds() / 3600\n",
    "        if 'offer_category' in group_df.columns: group_df['time_since_last_seen_this_category'] = group_df.groupby('offer_category')['id4'].diff().dt.total_seconds() / 3600\n",
    "        \n",
    "        group_df.drop(columns=['is_click_int'], inplace=True, errors='ignore')\n",
    "        group_df = downcast_dtypes(group_df)\n",
    "        output_path = os.path.join(output_dir, f\"part_{customer_id}.parquet\")\n",
    "        group_df.to_parquet(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # --- ENHANCED ERROR LOGGING (As per user request) ---\n",
    "        print(f\"\\n--- CRITICAL ERROR: Processing failed for customer ID: {customer_id} ---\")\n",
    "        error_type = type(e).__name__\n",
    "        print(f\"    Error Type: {error_type}\")\n",
    "        print(f\"    Error Message: {e}\")\n",
    "        # Try to load the data again just for logging purposes\n",
    "        try:\n",
    "            failing_df = pd.read_parquet(os.path.join(input_dir, f\"customer_{customer_id}.parquet\"))\n",
    "            print(f\"    Data shape for this customer: {failing_df.shape}\")\n",
    "            print(\"    Sample of failing customer data:\")\n",
    "            print(failing_df.head(5).to_string())\n",
    "            print(\"    Data types of failing customer data:\")\n",
    "            print(failing_df.info())\n",
    "        except Exception as log_e:\n",
    "            print(f\"    Could not reload data for logging: {log_e}\")\n",
    "        print(\"    Full Traceback:\")\n",
    "        traceback.print_exc()\n",
    "        print(f\"--- END ERROR LOG FOR CUSTOMER {customer_id} ---\\n\")\n",
    "        return False\n",
    "\n",
    "# --- Worker for Phase 3 - Parallel Assembly ---\n",
    "def combine_parts_worker(file_chunk, master_schema, output_dir):\n",
    "    \"\"\"\n",
    "    Reads a chunk of part files, concatenates them, and saves a single larger,\n",
    "    schema-consistent file by adding missing columns and casting to the master schema.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_list = [pd.read_parquet(f) for f in file_chunk]\n",
    "        chunk_df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        current_cols = set(chunk_df.columns)\n",
    "        for field in master_schema:\n",
    "            if field.name not in current_cols:\n",
    "                chunk_df[field.name] = pd.Series(dtype=field.type.to_pandas_dtype())\n",
    "\n",
    "        chunk_df = chunk_df[master_schema.names]\n",
    "        \n",
    "        table = pa.Table.from_pandas(chunk_df, schema=master_schema, preserve_index=False)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f\"combined_{uuid.uuid4()}.parquet\")\n",
    "        pq.write_table(table, output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in combining worker: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Main Feature Engineering Function ---\n",
    "def engineer_features_final(\n",
    "    df: pd.DataFrame, \n",
    "    output_path: str,\n",
    "    data_dict_path: str, \n",
    "    global_start_time=None,\n",
    "    offer_profile_features=None, # Removed customer_profile_features\n",
    "    schema_dict=None,\n",
    "    is_test_run=False\n",
    "):\n",
    "    print(f\"\\nProcessing DataFrame with shape: {df.shape} to create file: {output_path}\")\n",
    "    print(f\"Using {N_CORES} cores.\")\n",
    "    \n",
    "    for temp_dir in [TEMP_SPLIT_DIR, TEMP_PARTS_DIR, TEMP_COMBINED_DIR]:\n",
    "        if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
    "        os.makedirs(temp_dir)\n",
    "        print(f\"  -> Created temporary directory: {temp_dir}\")\n",
    "\n",
    "    data_dict = pd.read_csv(data_dict_path)\n",
    "    type_map = {row[\"masked_column\"]: row[\"Type\"].strip() for _, row in data_dict.iterrows()}\n",
    "    numerical_cols = [col for col in df.columns if col.startswith(\"f\") and type_map.get(col) == \"Numerical\"]\n",
    "    if 'f218' not in numerical_cols: numerical_cols.append('f218')\n",
    "    \n",
    "    print(\"Step 1/5: Engineering fast, vectorized features...\")\n",
    "    df[\"id4\"] = pd.to_datetime(df[\"id4\"])\n",
    "    df = df.sort_values(by=['id2', 'id4'])\n",
    "    time_diff_mins = df.groupby('id2')['id4'].diff().dt.total_seconds().div(60)\n",
    "    session_break = (time_diff_mins > 30).cumsum()\n",
    "    df['session_id'] = df['id2'].astype(str) + '_' + session_break.astype(str)\n",
    "    df.rename(columns={'f223': 'offer_age_days', 'f224': 'offer_time_to_expiry_days'}, inplace=True)\n",
    "    df['customer_account_age_days'] = (df['id4'] - df.groupby('id2')['id4'].transform('min')).dt.total_seconds() / (3600 * 24)\n",
    "    if global_start_time is None: global_start_time = df['id4'].min()\n",
    "    df['time_since_dataset_start_days'] = (df['id4'] - global_start_time).dt.total_seconds() / (3600 * 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['id4'].dt.day / 31); df['day_cos'] = np.cos(2 * np.pi * df['id4'].dt.day / 31)\n",
    "    category_cols = [f'f{i}' for i in range(226, 233)]; existing_cat_cols = [c for c in category_cols if c in df.columns]\n",
    "    if existing_cat_cols: df['offer_category'] = df[existing_cat_cols].idxmax(axis=1)\n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns: df[col] = np.floor(df[col] * 100) / 100\n",
    "    df['time_of_day_bin'] = pd.cut(df['id4'].dt.hour, bins=[-1, 5, 11, 17, 23], labels=[0, 1, 2, 3]).astype(np.int8)\n",
    "    df['is_weekend'] = (df['id4'].dt.weekday >= 5).astype(np.int8)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['id4'].dt.month / 12); df['month_cos'] = np.cos(2 * np.pi * df['id4'].dt.month / 12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['id4'].dt.dayofweek / 7); df['dayofweek_cos'] = np.cos(2 * np.pi * df['id4'].dt.dayofweek / 7)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['id4'].dt.hour / 24); df['hour_cos'] = np.cos(2 * np.pi * df['id4'].dt.hour / 24)\n",
    "    is_holiday = (df['id4'].dt.is_month_end & (df['id4'].dt.month == 12)) | (df['id4'].dt.is_month_start & (df['id4'].dt.month == 1))\n",
    "    df['is_holiday_week'] = is_holiday.astype(np.int8)\n",
    "    df['is_payday_week'] = ((df['id4'].dt.day >= 25) | (df['id4'].dt.day <= 5)).astype(np.int8)\n",
    "\n",
    "    # === BLOCK REMOVED: The leaky global customer profiles were calculated here ===\n",
    "    # This entire block was removed to prevent lookahead bias.\n",
    "    # The new dynamic profiles are calculated below.\n",
    "    \n",
    "    # --- Offer Profiles (Calculated once on training data, then reused) ---\n",
    "    if offer_profile_features is None:\n",
    "        print(\"Step 2/5: Computing offer profiles...\")\n",
    "        base_offer_profiles = df.groupby('id3').agg(\n",
    "            offer_popularity=('id1', 'count'), \n",
    "            offer_customer_reach=('id2', 'nunique')\n",
    "        )\n",
    "        monthly_counts = df.groupby([df['id4'].dt.to_period('M'), 'id3'])['id1'].count().unstack('id3').fillna(0)\n",
    "        if len(monthly_counts) >= 6:\n",
    "            growth_rate = (monthly_counts.iloc[-3:].mean() - monthly_counts.iloc[-6:-3].mean()) / (monthly_counts.iloc[-6:-3].mean() + 1e-6)\n",
    "            base_offer_profiles['offer_growth_rate'] = growth_rate\n",
    "        else:\n",
    "            base_offer_profiles['offer_growth_rate'] = 0\n",
    "        base_offer_profiles['offer_lifecycle_stage'] = pd.cut(base_offer_profiles['offer_growth_rate'], bins=[-np.inf, 0, 0.2, np.inf], labels=[0, 1, 2]).astype(np.int8)\n",
    "        offer_profile_features = base_offer_profiles\n",
    "\n",
    "    # --- Leakage-Free Historical & Dynamic Features ---\n",
    "    print(\"Step 2.5/5: Engineering leakage-free historical features...\")\n",
    "    \n",
    "    # Map the non-leaky offer profiles\n",
    "    for col in tqdm(offer_profile_features.columns, desc=\"  -> Mapping offer profiles\"): \n",
    "        df[col] = df['id3'].map(offer_profile_features[col])\n",
    "    \n",
    "    # --- FIX for cumsum TypeError on 'y' column ---\n",
    "    # The 'y' column in the test set is a placeholder, so this calculation is not meaningful for it.\n",
    "    # We also add a defensive type conversion to prevent dtype errors.\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "    if not is_test_run:\n",
    "        # Calculate historical CTR (leakage-free) for train/valid sets\n",
    "        offer_group = df.groupby('id3')\n",
    "        historical_clicks = offer_group['y'].cumsum().shift(1)\n",
    "        historical_views = offer_group.cumcount()\n",
    "        df['offer_historical_ctr_fixed'] = (historical_clicks / (historical_views + 1)).fillna(0)\n",
    "    else:\n",
    "        # For the test set, this feature doesn't make sense, so create a placeholder.\n",
    "        df['offer_historical_ctr_fixed'] = 0\n",
    "    \n",
    "    # Calculate popularity vs customer's historical norm (leakage-free)\n",
    "    cust_popularity_avg = df.groupby('id2')['offer_popularity'].expanding().mean().shift(1)\n",
    "    cust_popularity_avg = cust_popularity_avg.reset_index(level=0, drop=True)\n",
    "    df['popularity_vs_customer_norm'] = df['offer_popularity'] - cust_popularity_avg.fillna(df['offer_popularity'])\n",
    "    \n",
    "    # === NEW LEAKAGE-FREE BLOCK: Dynamic Customer Profiles ===\n",
    "    print(\"Step 2.7/5: Engineering dynamic customer profiles for all features...\")\n",
    "    \n",
    "    # Get all numerical and categorical columns for dynamic profiling\n",
    "    all_profile_cols = numerical_cols + [col for col in df.columns if col.startswith('cust_profile_') and 'nunique' in col]\n",
    "\n",
    "    for col in tqdm(all_profile_cols, desc=\"  -> Calculating dynamic profiles\"):\n",
    "        if col in df.columns:\n",
    "            customer_group = df.groupby('id2')[col]\n",
    "            \n",
    "            # Calculate expanding aggregates, shifting to prevent leakage from the current event\n",
    "            expanding_mean = customer_group.expanding().mean().shift(1)\n",
    "            expanding_std = customer_group.expanding().std().shift(1)\n",
    "            expanding_max = customer_group.expanding().max().shift(1)\n",
    "            expanding_min = customer_group.expanding().min().shift(1)\n",
    "            \n",
    "            # Reset index to align with the main dataframe\n",
    "            df[f'dynamic_{col}_mean'] = expanding_mean.reset_index(level=0, drop=True)\n",
    "            df[f'dynamic_{col}_std'] = expanding_std.reset_index(level=0, drop=True)\n",
    "            df[f'dynamic_{col}_max'] = expanding_max.reset_index(level=0, drop=True)\n",
    "            df[f'dynamic_{col}_min'] = expanding_min.reset_index(level=0, drop=True)\n",
    "\n",
    "    # Fill NaNs that result from the shift (i.e., the first event for each customer)\n",
    "    fill_cols = [c for c in df.columns if c.startswith('dynamic_')]\n",
    "    df[fill_cols] = df[fill_cols].fillna(0)\n",
    "    # === END OF NEW BLOCK ===\n",
    "\n",
    "    # --- PHASE 1: PARALLEL SPLITTING ---\n",
    "    print(f\"\\nStep 3/5: Pre-splitting data for {df['id2'].nunique()} customers in parallel...\")\n",
    "    customer_ids = df['id2'].unique()\n",
    "    id_chunks = np.array_split(customer_ids, N_CORES * 4)\n",
    "    split_func = partial(split_customer_data_worker, main_df=df, output_dir=TEMP_SPLIT_DIR)\n",
    "    with Pool(N_CORES) as p:\n",
    "        for _ in tqdm(p.imap_unordered(split_func, id_chunks), total=len(id_chunks), desc=\" -> Parallel Splitting Progress\"):\n",
    "            pass\n",
    "    del df; gc.collect()\n",
    "\n",
    "    # --- PHASE 2: PARALLEL FEATURE ENGINEERING ---\n",
    "    # --- DEFINITIVE FIX for FileNotFoundError: Process only the files that were actually created. ---\n",
    "    # This prevents race conditions or silent errors in the splitting phase from crashing the processing phase.\n",
    "    print(f\"Step 3.5/5: Discovering customer files from disk to process...\")\n",
    "    try:\n",
    "        customer_files = [f for f in os.listdir(TEMP_SPLIT_DIR) if f.startswith('customer_') and f.endswith('.parquet')]\n",
    "        customer_ids_from_files = [f.split('_')[1].split('.')[0] for f in customer_files]\n",
    "        print(f\"   -> Found {len(customer_ids_from_files)} customer files to process.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   -> ⚠️  Warning: Temporary split directory '{TEMP_SPLIT_DIR}' not found. Skipping feature engineering.\")\n",
    "        customer_ids_from_files = []\n",
    "\n",
    "    if customer_ids_from_files:\n",
    "        print(f\"Processing all {len(customer_ids_from_files)} customer files in parallel...\")\n",
    "        processing_func = partial(process_customer_file, input_dir=TEMP_SPLIT_DIR, output_dir=TEMP_PARTS_DIR, numerical_cols=numerical_cols)\n",
    "        with Pool(N_CORES) as p:\n",
    "            # Pass the list of IDs extracted from the actual files on disk\n",
    "            for _ in tqdm(p.imap_unordered(processing_func, customer_ids_from_files), total=len(customer_ids_from_files), desc=\" -> Feature Engineering Progress\"):\n",
    "                pass\n",
    "\n",
    "    if os.path.exists(TEMP_SPLIT_DIR):\n",
    "        shutil.rmtree(TEMP_SPLIT_DIR)\n",
    "\n",
    "    # --- Step 4/5: Robust, Parallel Assembly ---\n",
    "    print(\"\\nStep 4/5: Combining processed parts from disk...\")\n",
    "    part_files = [os.path.join(TEMP_PARTS_DIR, f) for f in os.listdir(TEMP_PARTS_DIR) if f.endswith('.parquet')]\n",
    "    if not part_files:\n",
    "        print(\"Warning: No temporary part files were created.\")\n",
    "        shutil.rmtree(TEMP_PARTS_DIR)\n",
    "        return None, offer_profile_features.reset_index(), {}, global_start_time\n",
    "    \n",
    "    if schema_dict is None:\n",
    "        print(\"  -> No master schema found. Creating a robust union schema from a sample of part files...\")\n",
    "        sample_files = part_files[:min(200, len(part_files))]\n",
    "        all_schemas = [pq.read_schema(f) for f in sample_files]\n",
    "        all_fields = {}\n",
    "        for schema in all_schemas:\n",
    "            for field in schema:\n",
    "                if field.name not in all_fields or pa.types.is_null(all_fields[field.name].type):\n",
    "                     all_fields[field.name] = field\n",
    "        master_schema_pa = pa.schema(list(all_fields.values()))\n",
    "        print(\"  -> Master schema created successfully.\")\n",
    "    else:\n",
    "        master_schema_pa = pa.schema(schema_dict)\n",
    "    \n",
    "    # --- PHASE 3: PARALLEL ASSEMBLY ---\n",
    "    print(f\"Step 4.5/5: Combining {len(part_files)} parts in parallel...\")\n",
    "    file_chunks = np.array_split(part_files, N_CORES * 4)\n",
    "    combine_func = partial(combine_parts_worker, master_schema=master_schema_pa, output_dir=TEMP_COMBINED_DIR)\n",
    "    with Pool(N_CORES) as p:\n",
    "        for _ in tqdm(p.imap_unordered(combine_func, file_chunks), total=len(file_chunks), desc=\" -> Combining Chunks Progress\"):\n",
    "            pass\n",
    "    shutil.rmtree(TEMP_PARTS_DIR)\n",
    "\n",
    "    # --- Final, fast combination of large chunks ---\n",
    "    print(\"\\nStep 5/5: Writing final output file...\")\n",
    "    combined_files = [os.path.join(TEMP_COMBINED_DIR, f) for f in os.listdir(TEMP_COMBINED_DIR)]\n",
    "    write_schema = master_schema_pa\n",
    "    if is_test_run and 'y' in write_schema.names:\n",
    "        print(\" -> Test run detected. Removing 'y' column from final output schema.\")\n",
    "        write_schema = write_schema.remove(write_schema.get_field_index('y'))\n",
    "    with pq.ParquetWriter(output_path, schema=write_schema) as writer:\n",
    "        for part_file in tqdm(combined_files, desc=\" -> Writing Final File\"):\n",
    "            table = pq.read_table(part_file)\n",
    "            if is_test_run and 'y' in table.column_names:\n",
    "                table = table.drop(['y'])\n",
    "            writer.write_table(table)\n",
    "            \n",
    "    shutil.rmtree(TEMP_COMBINED_DIR)\n",
    "    \n",
    "    print(f\"Finished creating file: {output_path}\")\n",
    "    final_schema_dict = {field.name: field.type for field in master_schema_pa}\n",
    "    # Return None for customer profiles as they are now part of the main df\n",
    "    return None, offer_profile_features.reset_index(), final_schema_dict, global_start_time\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 ***Script 2: Final Production-Grade Parallel Feature Engineering (Leakage Fixed) ***\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    if not SKIP_TRAINING:\n",
    "        print(\"\\n--- Processing Training Data ---\")\n",
    "        train_df = pd.read_parquet(f\"{INPUT_DIR}/train_1.parquet\")\n",
    "        \n",
    "        _, offer_profile, master_schema, global_start_time = engineer_features_final(\n",
    "            train_df, output_path=f\"{OUTPUT_DIR}/train_2_final.parquet\", data_dict_path=DATA_DICT_PATH\n",
    "        )\n",
    "        # customer_profile is no longer created or saved\n",
    "        offer_profile.to_parquet(f\"{OUTPUT_DIR}/offer_profile_aggs_final.parquet\")\n",
    "        del train_df; gc.collect()\n",
    "    else:\n",
    "        print(\"\\n--- Skipping Training Phase: Loading artifacts from disk... ---\")\n",
    "        offer_profile = pd.read_parquet(f\"{OUTPUT_DIR}/offer_profile_aggs_final.parquet\")\n",
    "        train_final_schema = pq.read_schema(f\"{OUTPUT_DIR}/train_2_final.parquet\")\n",
    "        master_schema = {field.name: field.type for field in train_final_schema}\n",
    "        train_pq_file = pq.ParquetFile(f\"{OUTPUT_DIR}/train_2_final.parquet\")\n",
    "        if 'id4' in train_pq_file.schema.names:\n",
    "            id4_col = train_pq_file.read(columns=['id4'])\n",
    "            global_start_time = id4_col['id4'].min().as_py()\n",
    "        else:\n",
    "            global_start_time = pd.Timestamp.now()\n",
    "        print(\" -> Artifacts loaded successfully.\")\n",
    "    \n",
    "    print(\"\\n--- Processing Validation Data ---\")\n",
    "    valid_df = pd.read_parquet(f\"{INPUT_DIR}/valid_1.parquet\")\n",
    "    \n",
    "    valid_df.reset_index(drop=True, inplace=True)\n",
    "    if 'y' in valid_df.columns:\n",
    "        valid_df['y'] = pd.to_numeric(valid_df['y'])\n",
    "    _, _, _, _ = engineer_features_final(\n",
    "        valid_df, output_path=f\"{OUTPUT_DIR}/valid_2_final.parquet\", data_dict_path=DATA_DICT_PATH, \n",
    "        global_start_time=global_start_time,\n",
    "        offer_profile_features=offer_profile.set_index('id3'), \n",
    "        schema_dict=master_schema\n",
    "    )\n",
    "    del valid_df; gc.collect()\n",
    "    \n",
    "    print(\"\\n--- Processing Test Data ---\")\n",
    "    test_file_path = f\"{INPUT_DIR}/test_1.parquet\"\n",
    "    if os.path.exists(test_file_path):\n",
    "        test_df = pd.read_parquet(test_file_path)\n",
    "        test_df.reset_index(drop=True, inplace=True)\n",
    "        if 'y' not in test_df.columns: test_df['y'] = -1\n",
    "        _, _, _, _ = engineer_features_final(\n",
    "            test_df, output_path=f\"{OUTPUT_DIR}/test_2_final.parquet\", data_dict_path=DATA_DICT_PATH, \n",
    "            global_start_time=global_start_time, \n",
    "            offer_profile_features=offer_profile.set_index('id3'), \n",
    "            schema_dict=master_schema,\n",
    "            is_test_run=True\n",
    "        )\n",
    "        del test_df, offer_profile\n",
    "    else:\n",
    "        print(\"\\n'test_1.parquet' not found. Skipping test set processing.\")\n",
    "    \n",
    "    print(\"\\n--- All datasets have been feature-engineered and saved successfully. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41295ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 *** Script 3 (Final): OOF Generation with LambdaMART ***\n",
      "\n",
      "--- Step 1: Loading feature-engineered data ---\n",
      "   -> Train shape: (589815, 3098)\n",
      "\n",
      "--- Step 2: Performing feature selection on 25.0% of customers ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   -> Converting categorical features to category dtype: 100%|██████████| 18/18 [00:00<00:00, 386.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6866, number of negative: 138538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.733138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 413003\n",
      "[LightGBM] [Info] Number of data points in the train set: 145404, number of used features: 2835\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047220 -> initscore=-3.004563\n",
      "[LightGBM] [Info] Start training from score -3.004563\n",
      "   -> Selected 100 features based on gain importance.\n",
      "\n",
      "--- Step 3: Preparing data with selected features for ranking ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   -> Converting categorical dtypes: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Generating OOF predictions with LambdaMART ---\n",
      "\n",
      "  -> Processing Fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's map@7: 0.962044\n",
      "[400]\tvalid_0's map@7: 0.963286\n",
      "Early stopping, best iteration is:\n",
      "[412]\tvalid_0's map@7: 0.963344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> Processing Fold 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's map@7: 0.959497\n",
      "[400]\tvalid_0's map@7: 0.960044\n",
      "[600]\tvalid_0's map@7: 0.960701\n",
      "Early stopping, best iteration is:\n",
      "[657]\tvalid_0's map@7: 0.961051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> Processing Fold 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's map@7: 0.964553\n",
      "[400]\tvalid_0's map@7: 0.965164\n",
      "Early stopping, best iteration is:\n",
      "[378]\tvalid_0's map@7: 0.965318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> Processing Fold 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's map@7: 0.961928\n",
      "[400]\tvalid_0's map@7: 0.962811\n",
      "[600]\tvalid_0's map@7: 0.963221\n",
      "[800]\tvalid_0's map@7: 0.963531\n",
      "Early stopping, best iteration is:\n",
      "[738]\tvalid_0's map@7: 0.963667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> Processing Fold 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's map@7: 0.962848\n",
      "[400]\tvalid_0's map@7: 0.96378\n",
      "[600]\tvalid_0's map@7: 0.964203\n",
      "[800]\tvalid_0's map@7: 0.964267\n",
      "[1000]\tvalid_0's map@7: 0.964374\n",
      "Early stopping, best iteration is:\n",
      "[1005]\tvalid_0's map@7: 0.964377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> OOF and Ensemble predictions created successfully.\n",
      "\n",
      "--- Step 5: Saving new datasets with OOF/Ensemble features ---\n",
      "   -> Saved 'train_3_oof.parquet' with shape (589815, 3099)\n",
      "   -> Saved 'valid_3_oof.parquet' with shape (180349, 3099)\n",
      "   -> Saved 'test_3_oof.parquet' with shape (337714, 3098)\n",
      "\n",
      "✅ --- Final OOF Feature Generation Complete --- ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"inter\"\n",
    "OUTPUT_DIR = \"inter\"\n",
    "DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "N_SPLITS = 5\n",
    "N_FEATURES_TO_SELECT = 100  # Select top 100 features based on gain\n",
    "SAMPLE_FRAC = 0.25  # Use 25% of customers for feature selection\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 *** Script 3 (Final): OOF Generation with LambdaMART ***\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 1. Load Data\n",
    "    # =================================================================\n",
    "    print(\"\\n--- Step 1: Loading feature-engineered data ---\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(f\"{INPUT_DIR}/train_2_final.parquet\")\n",
    "        valid_df = pd.read_parquet(f\"{INPUT_DIR}/valid_2_final.parquet\")\n",
    "        test_df = pd.read_parquet(f\"{INPUT_DIR}/test_2_final.parquet\")\n",
    "        print(f\"   -> Train shape: {train_df.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find input files. Details: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Preliminary Feature Selection (using 'gain' importance)\n",
    "    # =================================================================\n",
    "    print(\n",
    "        f\"\\n--- Step 2: Performing feature selection on {SAMPLE_FRAC * 100}% of customers ---\"\n",
    "    )\n",
    "\n",
    "    target_col = \"y\"\n",
    "    id_cols = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"]\n",
    "\n",
    "    data_dict = pd.read_csv(DATA_DICT_PATH)\n",
    "    all_cats = data_dict[data_dict[\"Type\"].str.strip() == \"Categorical\"][\n",
    "        \"masked_column\"\n",
    "    ].tolist()\n",
    "    new_cat_features = [\n",
    "        \"offer_category\",\n",
    "        \"session_id\",\n",
    "        \"time_of_day_bin\",\n",
    "        \"is_weekend\",\n",
    "        \"is_holiday_week\",\n",
    "        \"is_payday_week\",\n",
    "        \"offer_lifecycle_stage\",\n",
    "    ]\n",
    "\n",
    "    all_feature_cols = [\n",
    "        col for col in train_df.columns if col not in id_cols + [target_col]\n",
    "    ]\n",
    "    categorical_features = [\n",
    "        col for col in all_feature_cols if col in all_cats + new_cat_features\n",
    "    ]\n",
    "\n",
    "    customer_ids = train_df[\"id2\"].unique()\n",
    "    sample_customer_ids = (\n",
    "        pd.Series(customer_ids)\n",
    "        .sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "        .values\n",
    "    )\n",
    "    train_sample_df = train_df[train_df[\"id2\"].isin(sample_customer_ids)].copy()\n",
    "\n",
    "    for col in tqdm(categorical_features, desc=\"   -> Converting categorical features to category dtype\"):\n",
    "        if col in train_sample_df.columns:\n",
    "            train_sample_df[col] = train_sample_df[col].astype(\"category\")\n",
    "\n",
    "    fs_model = lgb.LGBMClassifier(\n",
    "        objective=\"binary\", random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    fs_model.fit(\n",
    "        train_sample_df[all_feature_cols],\n",
    "        train_sample_df[target_col],\n",
    "        categorical_feature=categorical_features,\n",
    "    )\n",
    "\n",
    "    # CRITICAL FIX: Use 'gain' for feature importance, not the default 'split'\n",
    "    importances = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": all_feature_cols,\n",
    "            \"importance_gain\": fs_model.booster_.feature_importance(\n",
    "                importance_type=\"gain\"\n",
    "            ),\n",
    "        }\n",
    "    ).sort_values(\"importance_gain\", ascending=False)\n",
    "\n",
    "    top_features = importances.head(N_FEATURES_TO_SELECT)[\"feature\"].tolist()\n",
    "    top_categorical_features = [f for f in top_features if f in categorical_features]\n",
    "\n",
    "    print(f\"   -> Selected {len(top_features)} features based on gain importance.\")\n",
    "    del fs_model, train_sample_df, importances\n",
    "    gc.collect()\n",
    "\n",
    "    # 3. Prepare DataFrames for LambdaMART\n",
    "    # =================================================================\n",
    "    print(\"\\n--- Step 3: Preparing data with selected features for ranking ---\")\n",
    "\n",
    "    # Sort all dataframes by customer and time to correctly calculate group sizes\n",
    "    train_df = train_df.sort_values(by=[\"id2\", \"id5\", \"id4\"])\n",
    "    valid_df = valid_df.sort_values(by=[\"id2\", \"id5\", \"id4\"])\n",
    "    test_df = test_df.sort_values(by=[\"id2\", \"id5\", \"id4\"])\n",
    "\n",
    "    X_train = train_df[top_features].copy()\n",
    "    y_train = train_df[target_col].copy()\n",
    "    groups = train_df[\"id2\"].copy()\n",
    "\n",
    "    X_valid = valid_df[top_features].copy()\n",
    "    X_test = test_df[top_features].copy()\n",
    "\n",
    "    for col in tqdm(\n",
    "        top_categorical_features, desc=\"   -> Converting categorical dtypes\"\n",
    "    ):\n",
    "        X_train[col] = X_train[col].astype(\"category\")\n",
    "        X_valid[col] = X_valid[col].astype(\"category\")\n",
    "        X_test[col] = X_test[col].astype(\"category\")\n",
    "\n",
    "    # Calculate group sizes for the ranking objective\n",
    "    train_group_sizes = train_df.groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "\n",
    "    # 4. OOF Generation with LambdaMART and StratifiedGroupKFold\n",
    "    # =================================================================\n",
    "    print(\"\\n--- Step 4: Generating OOF predictions with LambdaMART ---\")\n",
    "\n",
    "    lgbm_params = {\n",
    "        \"objective\": \"lambdarank\",  # <-- Use the ranking objective\n",
    "        \"metric\": \"map\",  # <-- Use the built-in, fast MAP metric\n",
    "        \"eval_at\": [7],  # <-- Tell the MAP metric to evaluate at k=7\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"num_leaves\": 40,\n",
    "        \"max_depth\": 7,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"subsample\": 0.7,\n",
    "        \"reg_alpha\": 0.1,\n",
    "        \"reg_lambda\": 0.1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(\n",
    "        n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    oof_preds = np.zeros(len(train_df))\n",
    "    valid_preds_ensemble = np.zeros(len(valid_df))\n",
    "    test_preds_ensemble = np.zeros(len(test_df))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        sgkf.split(X_train, y_train, groups=groups)\n",
    "    ):\n",
    "        print(f\"\\n  -> Processing Fold {fold + 1}/{N_SPLITS}...\")\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Get group sizes for the current fold's train and validation sets\n",
    "        train_fold_groups = (\n",
    "            train_df.iloc[train_idx].groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "        )\n",
    "        val_fold_groups = (\n",
    "            train_df.iloc[val_idx].groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "        )\n",
    "\n",
    "        model = lgb.LGBMRanker(**lgbm_params)\n",
    "        model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            group=train_fold_groups,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_group=[val_fold_groups],\n",
    "            eval_metric=\"map\",\n",
    "            callbacks=[\n",
    "                lgb.log_evaluation(period=200),\n",
    "                lgb.early_stopping(100, verbose=True),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Generate OOF predictions (it's a ranking score, not a probability)\n",
    "        fold_preds = model.predict(X_val_fold)\n",
    "        oof_preds[val_idx] = fold_preds\n",
    "\n",
    "        # Add to ensemble predictions for validation and test sets\n",
    "        valid_preds_ensemble += model.predict(X_valid) / N_SPLITS\n",
    "        test_preds_ensemble += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "    train_df[\"oof_lgbm_prediction\"] = oof_preds\n",
    "    valid_df[\"oof_lgbm_prediction\"] = valid_preds_ensemble\n",
    "    test_df[\"oof_lgbm_prediction\"] = test_preds_ensemble\n",
    "\n",
    "    print(\"\\n   -> OOF and Ensemble predictions created successfully.\")\n",
    "\n",
    "    # 5. Save the final datasets\n",
    "    # =================================================================\n",
    "    print(\"\\n--- Step 5: Saving new datasets with OOF/Ensemble features ---\")\n",
    "    train_df.to_parquet(f\"{OUTPUT_DIR}/train_3_oof.parquet\", index=False)\n",
    "    valid_df.to_parquet(f\"{OUTPUT_DIR}/valid_3_oof.parquet\", index=False)\n",
    "    test_df.to_parquet(f\"{OUTPUT_DIR}/test_3_oof.parquet\", index=False)\n",
    "\n",
    "    print(f\"   -> Saved 'train_3_oof.parquet' with shape {train_df.shape}\")\n",
    "    print(f\"   -> Saved 'valid_3_oof.parquet' with shape {valid_df.shape}\")\n",
    "    print(f\"   -> Saved 'test_3_oof.parquet' with shape {test_df.shape}\")\n",
    "\n",
    "    del train_df, valid_df, test_df, X_train, y_train, X_valid, X_test, groups\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\n✅ --- Final OOF Feature Generation Complete --- ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5054514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 *** Script 4: Feature Selection using Permutation Importance ***\n",
      "\n",
      "--- Step 1: Loading data ---\n",
      "\n",
      "--- Step 2: Defining features ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 73.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Training temporary LightGBM model ---\n",
      "   -> Temporary model saved to 'model/temp_model_for_importance.txt'\n",
      "\n",
      "============================================================\n",
      "🚀 *** Calculating GPU-Accelerated Permutation Importance *** 🚀\n",
      "============================================================\n",
      "\n",
      "1/5: Loading model and validation data to GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/treelite/model.py:143: FutureWarning: treelite.Model.load() is deprecated. Use treelite.frontend.load_lightgbm_model() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Converting categorical features to integer codes for GPU compatibility...\n",
      "   -> Filling NaNs with 0 for GPU inference compatibility...\n",
      "\n",
      "2/5: Creating Forest Inference (FIL) model...\n",
      "\n",
      "3/5: Calculating baseline validation AUC score...\n",
      "   -> Baseline AUC: 0.968592\n",
      "\n",
      "4/5: Calculating importances for 3093 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permuting Features: 100%|██████████| 3093/3093 [4:18:06<00:00,  5.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5/5: Processing results...\n",
      "✅ Permutation importance calculation complete.\n",
      "\n",
      "--- Step 5: Selected top 100 features with positive importance ---\n",
      "['oof_lgbm_prediction', 'time_since_last_event_hours', 'time_since_last_seen_this_category', 'dynamic_f210_mean', 'time_since_session_start_mins', 'f125hma5', 'f132', 'f366hma20', 'f132hma20', 'f210hma5', 'f366hma10', 'f125hma20', 'f365hma20', 'f366hma5', 'f210hma20']\n",
      "\n",
      "✅ Top feature list saved to 'model/top_100_features.json'\n",
      "\n",
      "--- Feature Selection Script Finished ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import uuid\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import json\n",
    "import matplotlib\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize_scalar\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import entropy\n",
    "import scipy.stats\n",
    "\n",
    "# Force Matplotlib to use a non-GUI backend\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "# --- SCRIPT 4: FEATURE SELECTION WITH PERMUTATION IMPORTANCE ---\n",
    "# This script trains a preliminary model to find the most important features.\n",
    "# It uses the GPU-accelerated permutation importance method for a robust measure\n",
    "# of feature impact and saves the top 1000 features for downstream models.\n",
    "\n",
    "# --- Helper Function for GPU-Accelerated Permutation Importance ---\n",
    "def calculate_permutation_importance_gpu(model_path, valid_df_pd, feature_cols, categorical_features, target_col, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Calculates and plots feature importance using GPU-accelerated permutation importance.\n",
    "    Returns a DataFrame of features and their importance scores.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚀 *** Calculating GPU-Accelerated Permutation Importance *** 🚀\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Import RAPIDS libraries\n",
    "    try:\n",
    "        import cudf\n",
    "        import cupy as cp\n",
    "        import treelite\n",
    "        from cuml import ForestInference\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Error: Missing RAPIDS libraries. Please ensure cuml, cudf, cupy, and treelite are installed. Details: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 2. Load model and data to GPU\n",
    "    print(\"\\n1/5: Loading model and validation data to GPU...\")\n",
    "    tl_model = treelite.Model.load(model_path, model_format='lightgbm')\n",
    "    \n",
    "    # --- FIX for object dtype error ---\n",
    "    # Create a purely numerical dataframe for GPU processing by converting categories to codes.\n",
    "    X_valid_pd_numeric = valid_df_pd[feature_cols].copy()\n",
    "    print(\"   -> Converting categorical features to integer codes for GPU compatibility...\")\n",
    "    for col in categorical_features:\n",
    "        if col in X_valid_pd_numeric.columns:\n",
    "            # The dtype is already 'category' from the main script block\n",
    "            X_valid_pd_numeric[col] = X_valid_pd_numeric[col].cat.codes\n",
    "            \n",
    "    X_valid_gdf = cudf.from_pandas(X_valid_pd_numeric)\n",
    "    # --- END FIX ---\n",
    "    \n",
    "    y_valid_gseries = cudf.from_pandas(valid_df_pd[target_col])\n",
    "    print(\"   -> Filling NaNs with 0 for GPU inference compatibility...\")\n",
    "    X_valid_gdf = X_valid_gdf.fillna(0) # FIL requires non-null data\n",
    "\n",
    "    # 3. Create FIL model for accelerated inference\n",
    "    print(\"\\n2/5: Creating Forest Inference (FIL) model...\")\n",
    "    fil_model = ForestInference.load_from_treelite_model(tl_model, is_classifier=True)\n",
    "\n",
    "    # 4. Calculate baseline score\n",
    "    print(\"\\n3/5: Calculating baseline validation AUC score...\")\n",
    "    baseline_preds = fil_model.predict_proba(X_valid_gdf)\n",
    "    baseline_score = roc_auc_score(y_valid_gseries.to_numpy(), baseline_preds.to_numpy())\n",
    "    print(f\"   -> Baseline AUC: {baseline_score:.6f}\")\n",
    "\n",
    "    # 5. Calculate permutation importance\n",
    "    print(f\"\\n4/5: Calculating importances for {len(feature_cols)} features...\")\n",
    "    importances = {}\n",
    "    for col in tqdm(feature_cols, desc=\"Permuting Features\"):\n",
    "        permuted_scores = []\n",
    "        original_col = X_valid_gdf[col].copy()\n",
    "        for _ in range(n_repeats):\n",
    "            shuffled_values = cp.random.permutation(original_col.to_cupy())\n",
    "            X_valid_gdf[col] = shuffled_values\n",
    "            permuted_preds = fil_model.predict_proba(X_valid_gdf)\n",
    "            permuted_score = roc_auc_score(y_valid_gseries.to_numpy(), permuted_preds.to_numpy())\n",
    "            permuted_scores.append(permuted_score)\n",
    "        X_valid_gdf[col] = original_col # Restore original column\n",
    "        importance = baseline_score - np.mean(permuted_scores)\n",
    "        importances[col] = importance\n",
    "\n",
    "    # 6. Process results\n",
    "    print(\"\\n5/5: Processing results...\")\n",
    "    importance_df = pd.DataFrame.from_dict(importances, orient='index', columns=['importance'])\n",
    "    importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    print(\"✅ Permutation importance calculation complete.\")\n",
    "    return importance_df\n",
    "\n",
    "# --- Main Execution for Feature Selection ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 *** Script 4: Feature Selection using Permutation Importance ***\")\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    INPUT_DIR = \"inter\"\n",
    "    MODEL_DIR = \"model\"\n",
    "    DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "    TOP_N_FEATURES = 100\n",
    "    \n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n--- Step 1: Loading data ---\")\n",
    "    train_df = pd.read_parquet(f\"{INPUT_DIR}/train_3_oof.parquet\")\n",
    "    valid_df = pd.read_parquet(f\"{INPUT_DIR}/valid_3_oof.parquet\")\n",
    "\n",
    "    # 2. Define features and target\n",
    "    print(\"\\n--- Step 2: Defining features ---\")\n",
    "    target_col = \"y\"\n",
    "    id_cols = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"]\n",
    "    feature_cols = [col for col in train_df.columns if col not in id_cols + [target_col]]\n",
    "    \n",
    "    data_dict = pd.read_csv(DATA_DICT_PATH)\n",
    "    all_cats = data_dict[data_dict[\"Type\"].str.strip() == \"Categorical\"][\"masked_column\"].tolist()\n",
    "    new_cat_features = ['offer_category','session_id','time_of_day_bin','is_weekend','is_holiday_week','is_payday_week','offer_lifecycle_stage','brand_id','industry_id']\n",
    "    categorical_features = [col for col in feature_cols if col in all_cats + new_cat_features]\n",
    "\n",
    "    for col in tqdm(categorical_features):\n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        valid_df[col] = valid_df[col].astype('category')\n",
    "        \n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_col]\n",
    "    X_valid = valid_df[feature_cols]\n",
    "    y_valid = valid_df[target_col]\n",
    "    \n",
    "    # 3. Train a temporary model\n",
    "    print(\"\\n--- Step 3: Training temporary LightGBM model ---\")\n",
    "    temp_params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000, 'learning_rate': 0.03, 'num_leaves': 40,\n",
    "        'max_depth': 6, 'seed': 42, 'n_jobs': -1, 'is_unbalance': True,\n",
    "        'colsample_bytree': 0.7, 'subsample': 0.7, 'verbose': -1\n",
    "    }\n",
    "    temp_model = lgb.LGBMClassifier(**temp_params)\n",
    "    temp_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n",
    "                   callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    \n",
    "    temp_model_path = f\"{MODEL_DIR}/temp_model_for_importance.txt\"\n",
    "    temp_model.booster_.save_model(temp_model_path)\n",
    "    print(f\"   -> Temporary model saved to '{temp_model_path}'\")\n",
    "    \n",
    "    del X_train, y_train, X_valid, y_valid, train_df; gc.collect()\n",
    "\n",
    "    # 4. Calculate permutation importance\n",
    "    importance_df = calculate_permutation_importance_gpu(\n",
    "        model_path=temp_model_path,\n",
    "        valid_df_pd=valid_df, # Pass the full validation dataframe\n",
    "        feature_cols=feature_cols,\n",
    "        categorical_features=categorical_features,\n",
    "        target_col=target_col,\n",
    "        n_repeats=3 # Use 3-5 repeats for a good balance of stability and speed\n",
    "    )\n",
    "\n",
    "    # 5. Select top features and save the list\n",
    "    if not importance_df.empty:\n",
    "        # Keep only features with positive importance\n",
    "        positive_importance_df = importance_df[importance_df['importance'] > 0]\n",
    "        top_features = positive_importance_df.head(TOP_N_FEATURES).index.tolist()\n",
    "        \n",
    "        # The OOF feature is critical, so we ensure it's included\n",
    "        if 'oof_lgbm_prediction' not in top_features:\n",
    "            top_features.append('oof_lgbm_prediction')\n",
    "            \n",
    "        print(f\"\\n--- Step 5: Selected top {len(top_features)} features with positive importance ---\")\n",
    "        print(top_features[:15]) # Print a sample\n",
    "        \n",
    "        selected_features_path = os.path.join(MODEL_DIR, \"top_100_features.json\")\n",
    "        with open(selected_features_path, 'w') as f:\n",
    "            json.dump(top_features, f, indent=4)\n",
    "        print(f\"\\n✅ Top feature list saved to '{selected_features_path}'\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Warning: Permutation importance calculation failed. Skipping feature selection.\")\n",
    "\n",
    "    print(\"\\n--- Feature Selection Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2503d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 *** Final Model Training with LambdaMART & Optuna ***\n",
      "\n",
      "1. Loading final OOF-featured datasets...\n",
      "\n",
      "2. Preparing data for ranking...\n",
      "   -> Found 42321 training groups and 10147 validation groups.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 10:57:15,882] A new study created in memory with name: no-name-48df6ab3-1373-4815-b2f7-a9d525edd5c1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Starting Optuna study with 50 trials to maximize MAP@7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81412572b5d4153903af011ea868132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 10:58:38,767] Trial 0 finished with value: 0.9523091283752422 and parameters: {'learning_rate': 0.04315490202135175, 'num_leaves': 120, 'max_depth': 7, 'subsample': 0.8124956402724836, 'colsample_bytree': 0.8502793825321208, 'reg_alpha': 2.5312632550260288e-08, 'reg_lambda': 2.601399743543997e-06}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 10:59:49,698] Trial 1 finished with value: 0.9518134170326584 and parameters: {'learning_rate': 0.009072685934457543, 'num_leaves': 95, 'max_depth': 5, 'subsample': 0.7781742062266135, 'colsample_bytree': 0.8420037402594945, 'reg_alpha': 0.006779590781597455, 'reg_lambda': 5.584138854344181e-07}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:01:02,421] Trial 2 finished with value: 0.952169959307924 and parameters: {'learning_rate': 0.007781020566505578, 'num_leaves': 33, 'max_depth': 6, 'subsample': 0.9307255693604418, 'colsample_bytree': 0.6612774405621021, 'reg_alpha': 0.00016191553924775503, 'reg_lambda': 9.731094679410602e-06}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:02:27,086] Trial 3 finished with value: 0.9518851622643734 and parameters: {'learning_rate': 0.010946088116799379, 'num_leaves': 41, 'max_depth': 4, 'subsample': 0.7565613721005133, 'colsample_bytree': 0.8362715589076154, 'reg_alpha': 0.28574613489469325, 'reg_lambda': 1.603283030939266}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:03:46,022] Trial 4 finished with value: 0.9515111785327119 and parameters: {'learning_rate': 0.007374499749018956, 'num_leaves': 57, 'max_depth': 6, 'subsample': 0.602766737704506, 'colsample_bytree': 0.7790151005096317, 'reg_alpha': 1.1983834902345488e-06, 'reg_lambda': 0.13678207998028383}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:04:57,685] Trial 5 finished with value: 0.9519864030497714 and parameters: {'learning_rate': 0.007905060734687892, 'num_leaves': 109, 'max_depth': 5, 'subsample': 0.7721616596117595, 'colsample_bytree': 0.8285734300971197, 'reg_alpha': 0.026920168038839117, 'reg_lambda': 2.1084125731335783}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:06:39,342] Trial 6 finished with value: 0.9519855778782058 and parameters: {'learning_rate': 0.008357342866276412, 'num_leaves': 36, 'max_depth': 9, 'subsample': 0.8062663282419631, 'colsample_bytree': 0.8428728879251387, 'reg_alpha': 1.484933485216028e-07, 'reg_lambda': 0.21545226160243008}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:07:46,997] Trial 7 finished with value: 0.9517952749905192 and parameters: {'learning_rate': 0.005643530076504742, 'num_leaves': 117, 'max_depth': 3, 'subsample': 0.8335033373172093, 'colsample_bytree': 0.7978496941065103, 'reg_alpha': 0.9624820253832884, 'reg_lambda': 2.4027249534912234}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:09:18,452] Trial 8 finished with value: 0.9522382423946218 and parameters: {'learning_rate': 0.02577929856334503, 'num_leaves': 102, 'max_depth': 10, 'subsample': 0.9442306416118061, 'colsample_bytree': 0.9422719552372593, 'reg_alpha': 7.126276986796462e-05, 'reg_lambda': 0.1467066289916334}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:10:26,113] Trial 9 finished with value: 0.9522568051234159 and parameters: {'learning_rate': 0.038032997837404416, 'num_leaves': 53, 'max_depth': 4, 'subsample': 0.6480803878932863, 'colsample_bytree': 0.6427370391259613, 'reg_alpha': 0.0001237299751717432, 'reg_lambda': 1.5780826928959951e-06}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:12:03,174] Trial 10 finished with value: 0.9522886287000594 and parameters: {'learning_rate': 0.04848890275128835, 'num_leaves': 147, 'max_depth': 12, 'subsample': 0.8736086578529093, 'colsample_bytree': 0.9818065289165737, 'reg_alpha': 1.4797766580838945e-08, 'reg_lambda': 0.001180024401116318}. Best is trial 0 with value: 0.9523091283752422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:13:40,603] Trial 11 finished with value: 0.9525725670511954 and parameters: {'learning_rate': 0.04804789542938092, 'num_leaves': 147, 'max_depth': 12, 'subsample': 0.8691225299211621, 'colsample_bytree': 0.992466716642839, 'reg_alpha': 1.0893575907089567e-08, 'reg_lambda': 0.0004113768971481198}. Best is trial 11 with value: 0.9525725670511954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:15:35,189] Trial 12 finished with value: 0.9523774852078081 and parameters: {'learning_rate': 0.02404105695072738, 'num_leaves': 147, 'max_depth': 8, 'subsample': 0.8776943448379959, 'colsample_bytree': 0.9333708193082186, 'reg_alpha': 9.296833384346104e-07, 'reg_lambda': 3.1215661252741404e-08}. Best is trial 11 with value: 0.9525725670511954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:17:11,992] Trial 13 finished with value: 0.952332355530169 and parameters: {'learning_rate': 0.021515624688891906, 'num_leaves': 149, 'max_depth': 12, 'subsample': 0.8899239489098313, 'colsample_bytree': 0.9275795535724093, 'reg_alpha': 2.793190385725638e-06, 'reg_lambda': 3.901449374760974e-08}. Best is trial 11 with value: 0.9525725670511954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:18:45,753] Trial 14 finished with value: 0.9524017246923735 and parameters: {'learning_rate': 0.030873770892089057, 'num_leaves': 130, 'max_depth': 9, 'subsample': 0.9790938689750094, 'colsample_bytree': 0.9154236728563686, 'reg_alpha': 2.5501799131083648e-06, 'reg_lambda': 0.0004775041577137679}. Best is trial 11 with value: 0.9525725670511954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:20:19,683] Trial 15 finished with value: 0.9525092657213339 and parameters: {'learning_rate': 0.030164885583534754, 'num_leaves': 129, 'max_depth': 10, 'subsample': 0.9983987811275791, 'colsample_bytree': 0.9918613617571379, 'reg_alpha': 1.647541656576636e-05, 'reg_lambda': 0.0006086779703423598}. Best is trial 11 with value: 0.9525725670511954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:22:13,624] Trial 16 finished with value: 0.952446980073196 and parameters: {'learning_rate': 0.01581373295865216, 'num_leaves': 76, 'max_depth': 10, 'subsample': 0.9988988345015171, 'colsample_bytree': 0.9842725082589511, 'reg_alpha': 0.0016196651008953584, 'reg_lambda': 0.00356637717852023}. Best is trial 11 with value: 0.9525725670511954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:23:47,128] Trial 17 finished with value: 0.9526481962319435 and parameters: {'learning_rate': 0.034200540828172386, 'num_leaves': 133, 'max_depth': 11, 'subsample': 0.710355199161106, 'colsample_bytree': 0.7242984823810963, 'reg_alpha': 1.1886589418043152e-05, 'reg_lambda': 3.63754825731392e-05}. Best is trial 17 with value: 0.9526481962319435.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:25:01,192] Trial 18 finished with value: 0.9523578123802331 and parameters: {'learning_rate': 0.017383212876987637, 'num_leaves': 78, 'max_depth': 11, 'subsample': 0.7106290503798076, 'colsample_bytree': 0.716986733106572, 'reg_alpha': 8.569630372827893, 'reg_lambda': 4.640141449275704e-05}. Best is trial 17 with value: 0.9526481962319435.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:26:34,557] Trial 19 finished with value: 0.9528524795483716 and parameters: {'learning_rate': 0.036209112810866244, 'num_leaves': 134, 'max_depth': 12, 'subsample': 0.7088490956883309, 'colsample_bytree': 0.7272748855806079, 'reg_alpha': 9.128461832526685e-08, 'reg_lambda': 0.00951995771808995}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:28:05,684] Trial 20 finished with value: 0.9522687787706652 and parameters: {'learning_rate': 0.03578003040892951, 'num_leaves': 130, 'max_depth': 11, 'subsample': 0.7135933175566118, 'colsample_bytree': 0.7227223561025166, 'reg_alpha': 9.419953447297858e-08, 'reg_lambda': 0.01189867331695327}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:30:13,832] Trial 21 finished with value: 0.9523344533766334 and parameters: {'learning_rate': 0.04684747003556021, 'num_leaves': 138, 'max_depth': 12, 'subsample': 0.7006495866759099, 'colsample_bytree': 0.7413687006413867, 'reg_alpha': 4.1511415206850547e-07, 'reg_lambda': 8.05077225085035e-05}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:31:47,386] Trial 22 finished with value: 0.952644927390489 and parameters: {'learning_rate': 0.035570347891136875, 'num_leaves': 115, 'max_depth': 11, 'subsample': 0.6700206018387984, 'colsample_bytree': 0.6966651188637216, 'reg_alpha': 1.8720779403244957e-05, 'reg_lambda': 0.014002795151006634}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:33:58,758] Trial 23 finished with value: 0.9520054551829596 and parameters: {'learning_rate': 0.01979112064147624, 'num_leaves': 114, 'max_depth': 11, 'subsample': 0.666121197495584, 'colsample_bytree': 0.6072429810110487, 'reg_alpha': 1.8928416040217657e-05, 'reg_lambda': 0.05751289235551308}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:35:24,511] Trial 24 finished with value: 0.9521271699442238 and parameters: {'learning_rate': 0.012224555510484023, 'num_leaves': 93, 'max_depth': 9, 'subsample': 0.6408264292826178, 'colsample_bytree': 0.670757944616533, 'reg_alpha': 1.2318787440905712e-05, 'reg_lambda': 0.006891086593720632}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:36:54,531] Trial 25 finished with value: 0.9522039483314101 and parameters: {'learning_rate': 0.029973487529453564, 'num_leaves': 124, 'max_depth': 11, 'subsample': 0.7423910981103219, 'colsample_bytree': 0.756094298065756, 'reg_alpha': 0.002691516260296913, 'reg_lambda': 4.9557367511087474e-05}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:38:19,079] Trial 26 finished with value: 0.9525253205319446 and parameters: {'learning_rate': 0.037120462788094735, 'num_leaves': 109, 'max_depth': 8, 'subsample': 0.6860410936973571, 'colsample_bytree': 0.6949772002035963, 'reg_alpha': 6.823067658211402e-06, 'reg_lambda': 0.0288308531163257}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:39:49,448] Trial 27 finished with value: 0.9523464443429882 and parameters: {'learning_rate': 0.02677357953637676, 'num_leaves': 137, 'max_depth': 10, 'subsample': 0.6099715326519156, 'colsample_bytree': 0.6984641901911557, 'reg_alpha': 0.0007432130939223513, 'reg_lambda': 0.0016753198066994288}. Best is trial 19 with value: 0.9528524795483716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:42:10,284] Trial 28 finished with value: 0.9529399801377797 and parameters: {'learning_rate': 0.03569272548411728, 'num_leaves': 138, 'max_depth': 11, 'subsample': 0.7357596723843407, 'colsample_bytree': 0.6145306410975434, 'reg_alpha': 2.320386769070152e-07, 'reg_lambda': 0.0001044178863479317}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:43:41,571] Trial 29 finished with value: 0.9525999094490134 and parameters: {'learning_rate': 0.04002547345762381, 'num_leaves': 136, 'max_depth': 12, 'subsample': 0.7342068026214176, 'colsample_bytree': 0.6230698703063667, 'reg_alpha': 1.424588982708016e-07, 'reg_lambda': 6.630517967721582e-06}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:45:17,203] Trial 30 finished with value: 0.9516693282220744 and parameters: {'learning_rate': 0.012874690341984575, 'num_leaves': 22, 'max_depth': 9, 'subsample': 0.8195944666075786, 'colsample_bytree': 0.7633518211951075, 'reg_alpha': 5.8776692575908976e-08, 'reg_lambda': 1.772206062975433e-07}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:47:29,247] Trial 31 finished with value: 0.9528484453141989 and parameters: {'learning_rate': 0.03308930374721198, 'num_leaves': 122, 'max_depth': 11, 'subsample': 0.6691719354501987, 'colsample_bytree': 0.6003700926168187, 'reg_alpha': 2.9607235473389417e-05, 'reg_lambda': 1.595341666889147e-05}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:48:58,607] Trial 32 finished with value: 0.952603269355441 and parameters: {'learning_rate': 0.03238516112314817, 'num_leaves': 123, 'max_depth': 11, 'subsample': 0.72610539279207, 'colsample_bytree': 0.6341766994626048, 'reg_alpha': 2.717735653275386e-07, 'reg_lambda': 0.00010184394077153882}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:50:58,284] Trial 33 finished with value: 0.9525918197507968 and parameters: {'learning_rate': 0.0425376294585695, 'num_leaves': 100, 'max_depth': 10, 'subsample': 0.7704608021641326, 'colsample_bytree': 0.6017986579107264, 'reg_alpha': 3.638874790461442e-08, 'reg_lambda': 1.1382260675018496e-05}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:52:31,471] Trial 34 finished with value: 0.9529289880927241 and parameters: {'learning_rate': 0.023401586655966592, 'num_leaves': 140, 'max_depth': 12, 'subsample': 0.6405103431491939, 'colsample_bytree': 0.6663622575964607, 'reg_alpha': 8.495009405357964e-05, 'reg_lambda': 1.2707422249593633e-06}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:54:04,436] Trial 35 finished with value: 0.9527906760417776 and parameters: {'learning_rate': 0.022698964863646552, 'num_leaves': 140, 'max_depth': 12, 'subsample': 0.6274333682509718, 'colsample_bytree': 0.661113101597756, 'reg_alpha': 0.00031452956999722544, 'reg_lambda': 1.8343191511981384e-06}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:55:23,274] Trial 36 finished with value: 0.9521937574346452 and parameters: {'learning_rate': 0.027289555207004934, 'num_leaves': 121, 'max_depth': 7, 'subsample': 0.6785535611515847, 'colsample_bytree': 0.665398571360921, 'reg_alpha': 5.088658507421099e-05, 'reg_lambda': 3.766842441942878e-07}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:56:58,003] Trial 37 finished with value: 0.952219823246798 and parameters: {'learning_rate': 0.018922618165424298, 'num_leaves': 139, 'max_depth': 12, 'subsample': 0.6542404315200889, 'colsample_bytree': 0.6465479676561875, 'reg_alpha': 9.750253824455401e-07, 'reg_lambda': 1.0847270806390162e-05}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 11:58:38,543] Trial 38 finished with value: 0.9525626834288787 and parameters: {'learning_rate': 0.04250780201959497, 'num_leaves': 124, 'max_depth': 6, 'subsample': 0.7552167197884667, 'colsample_bytree': 0.6167780434975253, 'reg_alpha': 0.01600022639040221, 'reg_lambda': 0.00015768167461561582}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:00:33,345] Trial 39 finished with value: 0.9523453990511812 and parameters: {'learning_rate': 0.0269935554750068, 'num_leaves': 88, 'max_depth': 10, 'subsample': 0.6225902347293852, 'colsample_bytree': 0.6786461332707345, 'reg_alpha': 0.0004016178567714561, 'reg_lambda': 1.0113543260104355e-08}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:01:57,630] Trial 40 finished with value: 0.9522649613046492 and parameters: {'learning_rate': 0.020905028585990918, 'num_leaves': 107, 'max_depth': 11, 'subsample': 0.7838606419163239, 'colsample_bytree': 0.6432734863696742, 'reg_alpha': 0.04861047971751806, 'reg_lambda': 9.678779166674262}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:03:32,913] Trial 41 finished with value: 0.9526925063248254 and parameters: {'learning_rate': 0.02317912164477795, 'num_leaves': 141, 'max_depth': 12, 'subsample': 0.6322851369237664, 'colsample_bytree': 0.657802404201342, 'reg_alpha': 0.00023481627789532136, 'reg_lambda': 1.7568658402492515e-06}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:05:15,019] Trial 42 finished with value: 0.9520256112694413 and parameters: {'learning_rate': 0.02383107587054205, 'num_leaves': 142, 'max_depth': 12, 'subsample': 0.6023864627483164, 'colsample_bytree': 0.6304478821161954, 'reg_alpha': 5.170444110127688e-05, 'reg_lambda': 3.403323939208628e-06}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:06:51,614] Trial 43 finished with value: 0.9528455938073136 and parameters: {'learning_rate': 0.029344435540684836, 'num_leaves': 150, 'max_depth': 12, 'subsample': 0.6843720917386434, 'colsample_bytree': 0.6806942166886923, 'reg_alpha': 0.0015634241832383998, 'reg_lambda': 3.4782217670464973e-07}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:09:02,379] Trial 44 finished with value: 0.9522235323287355 and parameters: {'learning_rate': 0.02818543197334092, 'num_leaves': 149, 'max_depth': 11, 'subsample': 0.6587549843632775, 'colsample_bytree': 0.8697706212147718, 'reg_alpha': 0.0035333036501822247, 'reg_lambda': 6.225806628916447e-07}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:10:36,596] Trial 45 finished with value: 0.9524109094049891 and parameters: {'learning_rate': 0.0330517151801924, 'num_leaves': 128, 'max_depth': 12, 'subsample': 0.6787547746709942, 'colsample_bytree': 0.799687896792756, 'reg_alpha': 0.08290511800757981, 'reg_lambda': 1.0440267425690874e-07}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:12:56,876] Trial 46 finished with value: 0.952490202414529 and parameters: {'learning_rate': 0.03855197411499276, 'num_leaves': 144, 'max_depth': 12, 'subsample': 0.6905603184269128, 'colsample_bytree': 0.6897078379151721, 'reg_alpha': 0.0007878409800532735, 'reg_lambda': 2.704394357161711e-05}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:14:51,680] Trial 47 finished with value: 0.9521129213486913 and parameters: {'learning_rate': 0.005730828961920902, 'num_leaves': 62, 'max_depth': 10, 'subsample': 0.7443726518057666, 'colsample_bytree': 0.6010101045668377, 'reg_alpha': 0.0065609781206037495, 'reg_lambda': 5.615468156694983e-07}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:16:24,776] Trial 48 finished with value: 0.9525652902335667 and parameters: {'learning_rate': 0.0445961198902304, 'num_leaves': 133, 'max_depth': 5, 'subsample': 0.7236032474909819, 'colsample_bytree': 0.6221579347916506, 'reg_alpha': 2.4068051798427375e-08, 'reg_lambda': 0.5397195235148253}. Best is trial 28 with value: 0.9529399801377797.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 12:17:30,513] Trial 49 finished with value: 0.9517336295905965 and parameters: {'learning_rate': 0.024749617888519218, 'num_leaves': 143, 'max_depth': 3, 'subsample': 0.6984455869925298, 'colsample_bytree': 0.7100223452063918, 'reg_alpha': 2.778459337606068e-06, 'reg_lambda': 0.0002216319671853194}. Best is trial 28 with value: 0.9529399801377797.\n",
      "\n",
      "--- Optimization Finished ---\n",
      "  Value (MAP@7): 0.952940\n",
      "  Best Params: \n",
      "    learning_rate: 0.03569272548411728\n",
      "    num_leaves: 138\n",
      "    max_depth: 11\n",
      "    subsample: 0.7357596723843407\n",
      "    colsample_bytree: 0.6145306410975434\n",
      "    reg_alpha: 2.320386769070152e-07\n",
      "    reg_lambda: 0.0001044178863479317\n",
      "\n",
      "Best ranking parameters saved to model/best_lgbm_ranker_params.json\n",
      "\n",
      "5. Re-training final ranker model with best parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's map@7: 0.952893\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid_0's map@7: 0.953338\n",
      "\n",
      "Saving final model and artifacts...\n",
      "   -> Model and feature importance plot saved successfully.\n",
      "\n",
      "Calculating final validation MAP@7 score with optimized ranker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "📊 Final Optimized Validation MAP@7 Score: 0.095106\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Script finished successfully. 🎉\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4681/674900851.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    }
   ],
   "source": [
    "# --- THE DEFINITIVE FIX for the \"ArrowDtype\" error ---\n",
    "# This block MUST be at the very top of the script, before any other imports.\n",
    "import sys\n",
    "if 'dask' in sys.modules:\n",
    "    del sys.modules['dask']\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import matplotlib\n",
    "\n",
    "# Force Matplotlib to use a non-GUI backend to prevent errors in certain environments\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "INTER_DIR = \"inter\"\n",
    "MODEL_DIR = \"model\"\n",
    "DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "# We use the final, most feature-rich datasets for tuning\n",
    "TRAIN_FILE = \"inter/train_3_oof.parquet\"\n",
    "VALID_FILE = \"inter/valid_3_oof.parquet\"\n",
    "# Number of optimization trials to run. More is better, but takes longer.\n",
    "N_TRIALS = 50\n",
    "RANDOM_STATE = 42 \n",
    "\n",
    "# --- Helper Function for Metric ---\n",
    "# We use AUC as the optimization metric because it's faster than MAP@7 and highly correlated.\n",
    "def calculate_map_at_7(y_true, y_pred_probs, ids_df):\n",
    "    \"\"\"Calculates Mean Average Precision at 7 for the Amex competition.\"\"\"\n",
    "    \n",
    "    def ap_at_k(group, k=7):\n",
    "        \"\"\"Calculates Average Precision at k for a single group.\"\"\"\n",
    "        group = group.sort_values('pred', ascending=False)\n",
    "        y_true_sorted = group['y'].values\n",
    "        \n",
    "        # Truncate at k\n",
    "        y_true_sorted = y_true_sorted[:k]\n",
    "        \n",
    "        relevance = (y_true_sorted == 1)\n",
    "        if np.sum(relevance) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        precision_at_i = np.cumsum(relevance) / (np.arange(len(relevance)) + 1)\n",
    "        ap = np.sum(precision_at_i * relevance) / np.sum(relevance)\n",
    "        return ap\n",
    "\n",
    "    # Create a temporary DataFrame for calculation\n",
    "    calc_df = ids_df[['id2', 'id5']].copy()\n",
    "    calc_df['y'] = y_true\n",
    "    calc_df['pred'] = y_pred_probs\n",
    "\n",
    "    # --- SCORING SCRIPT FIX ---\n",
    "    # Convert id5 to date to match official scoring logic\n",
    "    calc_df['id5'] = pd.to_datetime(calc_df['id5']).dt.date\n",
    "    \n",
    "    # Calculate AP for each customer-session group\n",
    "    ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n",
    "    \n",
    "    # Return the mean of the AP scores\n",
    "    return ap_scores.mean()\n",
    "\n",
    "# --- Optuna Objective Function for LambdaMART ---\n",
    "def objective(trial, X_train, y_train, train_groups, X_valid, y_valid, valid_groups, categorical_features):\n",
    "    \"\"\"\n",
    "    Optuna objective function to tune hyperparameters for a LightGBM LambdaMART model.\n",
    "    \"\"\"\n",
    "    # Define the search space for the hyperparameters\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'map',\n",
    "        'eval_at': [7],\n",
    "        'seed': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRanker(**params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        group=train_groups,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_group=[valid_groups],\n",
    "        eval_metric='map',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Return the best MAP@7 score found during training\n",
    "    map_score = model.best_score_['valid_0']['map@7']\n",
    "    return map_score\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"🚀 *** Final Model Training with LambdaMART & Optuna ***\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"\\n1. Loading final OOF-featured datasets...\")\n",
    "    train_df = pd.read_parquet(TRAIN_FILE)\n",
    "    valid_df = pd.read_parquet(VALID_FILE)\n",
    "    \n",
    "    # 2. Prepare Data for LambdaMART\n",
    "    print(\"\\n2. Preparing data for ranking...\")\n",
    "    \n",
    "    # CRITICAL: Sort data by group keys to ensure correct group calculation\n",
    "    train_df = train_df.sort_values(by=['id2', 'id5', 'id4'])\n",
    "    valid_df = valid_df.sort_values(by=['id2', 'id5', 'id4'])\n",
    "\n",
    "    target_col = \"y\"\n",
    "    id_cols = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"]\n",
    "    feature_cols = [col for col in train_df.columns if col not in id_cols + [target_col]]\n",
    "    \n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    y_train = train_df[target_col].copy()\n",
    "    X_valid = valid_df[feature_cols].copy()\n",
    "    y_valid = valid_df[target_col].copy()\n",
    "    \n",
    "    # Calculate group sizes for the ranker\n",
    "    train_groups = train_df.groupby(['id2', 'id5']).size().to_numpy()\n",
    "    valid_groups = valid_df.groupby(['id2', 'id5']).size().to_numpy()\n",
    "    print(f\"   -> Found {len(train_groups)} training groups and {len(valid_groups)} validation groups.\")\n",
    "\n",
    "    data_dict = pd.read_csv(DATA_DICT_PATH)\n",
    "    all_cats = data_dict[data_dict[\"Type\"].str.strip() == \"Categorical\"][\"masked_column\"].tolist()\n",
    "    new_cat_features = [\n",
    "        'offer_category', 'session_id', 'time_of_day_bin', 'is_weekend',\n",
    "        'is_holiday_week', 'is_payday_week', 'offer_lifecycle_stage'\n",
    "    ]\n",
    "    categorical_features = [col for col in feature_cols if col in all_cats + new_cat_features]\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_valid[col] = X_valid[col].astype('category')\n",
    "    \n",
    "    valid_df_ids = valid_df[['id2', 'id5']].copy()\n",
    "    \n",
    "    # Clean up memory\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    # 3. Run Optuna Study\n",
    "    print(f\"\\n3. Starting Optuna study with {N_TRIALS} trials to maximize MAP@7...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, train_groups, X_valid, y_valid, valid_groups, categorical_features),\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True \n",
    "    )\n",
    "\n",
    "    # 4. Print and Save Results\n",
    "    print(\"\\n--- Optimization Finished ---\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  Value (MAP@7): {best_trial.value:.6f}\")\n",
    "    print(\"  Best Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "        \n",
    "    best_params_path = os.path.join(MODEL_DIR, \"best_lgbm_ranker_params.json\")\n",
    "    with open(best_params_path, 'w') as f:\n",
    "        json.dump(best_trial.params, f, indent=4)\n",
    "    print(f\"\\nBest ranking parameters saved to {best_params_path}\")\n",
    "\n",
    "    # 5. Re-train final model with best params\n",
    "    print(\"\\n5. Re-training final ranker model with best parameters...\")\n",
    "    best_params = best_trial.params\n",
    "    # Add non-tuned parameters\n",
    "    best_params['objective'] = 'lambdarank'\n",
    "    best_params['metric'] = 'map'\n",
    "    best_params['eval_at'] = [7]\n",
    "    best_params['seed'] = RANDOM_STATE\n",
    "    best_params['n_jobs'] = -1\n",
    "    best_params['verbose'] = -1\n",
    "    best_params['n_estimators'] = 4000 # Use a high number for final training with early stopping\n",
    "    \n",
    "    final_model = lgb.LGBMRanker(**best_params)\n",
    "    \n",
    "    final_model.fit(\n",
    "        X_train, y_train,\n",
    "        group=train_groups,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_group=[valid_groups],\n",
    "        eval_metric='map',\n",
    "        callbacks=[\n",
    "            lgb.log_evaluation(period=200),\n",
    "            lgb.early_stopping(100, verbose=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # --- Artifact Saving ---\n",
    "    print(\"\\nSaving final model and artifacts...\")\n",
    "    final_model.booster_.save_model(f\"{MODEL_DIR}/lgbm_final_ranker_model.txt\")\n",
    "    \n",
    "    # Plot feature importance using the correct method for a fitted model\n",
    "    lgb.plot_importance(final_model.booster_, max_num_features=30, figsize=(10, 15), importance_type='gain')\n",
    "    plt.title('Tuned Ranker Feature Importance (Top 30 by Gain)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_DIR}/feature_importance_ranker_tuned.png\")\n",
    "    plt.close() \n",
    "    print(\"   -> Model and feature importance plot saved successfully.\")\n",
    "    \n",
    "    print(\"\\nCalculating final validation MAP@7 score with optimized ranker...\")\n",
    "    valid_preds_scores = final_model.predict(X_valid)\n",
    "    map_at_7_score = calculate_map_at_7(y_valid.values, valid_preds_scores, valid_df_ids)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"📊 Final Optimized Validation MAP@7 Score: {map_at_7_score:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\n✅ Script finished successfully. 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e6e9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 === SCRIPT 1: Generating Base LGBM Predictions ===\n",
      "================================================================================\n",
      "\n",
      "1/4: Loading model, test data, and submission template...\n",
      "   -> Artifacts loaded successfully.\n",
      "   -> Test data shape: (337714, 3098)\n",
      "\n",
      "2/4: Preparing test data and aligning features...\n",
      "   -> Model was trained on 3093 features.\n",
      "   -> Found 18 categorical features to convert.\n",
      "   -> Matching categorical dtypes: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Test features aligned. Final shape for prediction: (337714, 3093)\n",
      "\n",
      "3/4: Generating predictions on the test set...\n",
      "   -> Predictions generated.\n",
      "\n",
      "4/4: Mapping predictions to submission format...\n",
      "\n",
      "==================================================\n",
      "✅ LGBM base predictions saved successfully: 'lgbm_test_preds.csv'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "\n",
    "# =================================================================================\n",
    "# 🚀 SCRIPT 1: GENERATE BASE PREDICTIONS (LGBM)\n",
    "# =================================================================================\n",
    "# This part of the script is responsible for loading a pre-trained LightGBM model\n",
    "# and using it to generate predictions on the test set. These predictions will\n",
    "# serve as the base for the subsequent residual modeling stage.\n",
    "\n",
    "def generate_lgbm_predictions():\n",
    "    \"\"\"\n",
    "    Loads a pre-trained LightGBM ranker model and generates predictions on the\n",
    "    final test set. This creates a base prediction file that can be used\n",
    "    for ensembling later.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 1: Generating Base LGBM Predictions ===\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # --- 1. Load Artifacts ---\n",
    "    print(\"1/4: Loading model, test data, and submission template...\")\n",
    "    try:\n",
    "        model = lgb.Booster(model_file=\"model/lgbm_final_ranker_model.txt\")\n",
    "        # Load the full datasets which contain all features the model was trained on\n",
    "        train_df = pd.read_parquet(\"inter/train_3_oof.parquet\")\n",
    "        test_df = pd.read_parquet(\"inter/test_3_oof.parquet\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error: Could not find a required model/data file. Details: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"   -> Artifacts loaded successfully.\")\n",
    "    print(f\"   -> Test data shape: {test_df.shape}\")\n",
    "\n",
    "    # --- 2. Prepare Test Data ---\n",
    "    print(\"\\n2/4: Preparing test data and aligning features...\")\n",
    "    \n",
    "    # --- FIX: Use ALL features for prediction, consistent with the final training step ---\n",
    "    # The final model was trained on all available features, so we must use them all here.\n",
    "    id_cols = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"]\n",
    "    features_to_use = [col for col in test_df.columns if col not in id_cols + ['y']]\n",
    "    print(f\"   -> Model was trained on {len(features_to_use)} features.\")\n",
    "    \n",
    "    # The saved parquet file may not retain the 'category' dtype. We must redefine them\n",
    "    # using the same logic as in the training script to ensure a perfect match.\n",
    "    DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "    data_dict = pd.read_csv(DATA_DICT_PATH)\n",
    "    all_cats = data_dict[data_dict[\"Type\"].str.strip() == \"Categorical\"][\"masked_column\"].tolist()\n",
    "    new_cat_features = [\n",
    "        \"offer_category\", \"session_id\", \"time_of_day_bin\", \"is_weekend\",\n",
    "        \"is_holiday_week\", \"is_payday_week\", \"offer_lifecycle_stage\"\n",
    "    ]\n",
    "    all_possible_cats = all_cats + new_cat_features\n",
    "    categorical_features = [f for f in features_to_use if f in all_possible_cats]\n",
    "    print(f\"   -> Found {len(categorical_features)} categorical features to convert.\")\n",
    "\n",
    "    print(f\"   -> Matching categorical dtypes: \", end=\"\")\n",
    "    for col in tqdm(categorical_features, leave=False):\n",
    "        # Ensure the column exists before trying to convert it\n",
    "        if col in train_df.columns and col in test_df.columns:\n",
    "            train_cat_dtype = train_df[col].astype('category').dtype\n",
    "            test_df[col] = test_df[col].astype(train_cat_dtype)\n",
    "    \n",
    "    # Final feature alignment\n",
    "    X_test = test_df[features_to_use]\n",
    "    print(f\"   -> Test features aligned. Final shape for prediction: {X_test.shape}\")\n",
    "    \n",
    "    # --- 3. Generate Predictions ---\n",
    "    print(\"\\n3/4: Generating predictions on the test set...\")\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"   -> Predictions generated.\")\n",
    "    \n",
    "    # --- 4. Create Submission File ---\n",
    "    print(\"\\n4/4: Mapping predictions to submission format...\")\n",
    "    \n",
    "    # --- SCORING SCRIPT FIX ---\n",
    "    # Include id2, id3, and id5 for compatibility with the scoring script and final merge\n",
    "    submission_df = test_df[['id1', 'id2', 'id3', 'id5']].copy()\n",
    "    submission_df['pred'] = predictions\n",
    "    \n",
    "    output_path = 'lgbm_test_preds.csv'\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"✅ LGBM base predictions saved successfully: '{output_path}'\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    del test_df, train_df, X_test\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # STEP 1: Generate base predictions from the final tuned LGBM model.\n",
    "    generate_lgbm_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec4f5584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 === SCRIPT 2: RESIDUAL TRANSFORMER PIPELINE ===\n",
      "================================================================================\n",
      "\n",
      "--- Preparing data for Advanced Transformer (Residual Modeling) ---\n",
      "   -> Creating residual target (y - lgbm_oof_prediction)...\n",
      "\n",
      "   -> Attempting to load list of top features for Transformer...\n",
      "   -> ✅ Successfully loaded 100 selected features from 'model/top_100_features.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   -> Encoding Categoricals (Leakage-Proof): 100%|██████████| 2/2 [00:00<00:00, 14.80it/s]\n",
      "Creating sequences (train): 100%|██████████| 38559/38559 [00:40<00:00, 956.11it/s] \n",
      "Creating sequences (test): 100%|██████████| 18956/18956 [00:18<00:00, 1025.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== FOLD 1/5 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -> Train Loss: 3.386113 | Val MAP@7: 0.071326\n",
      "   -> 🎉 New best score! Model for Fold 1 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 -> Train Loss: 2.500092 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 -> Train Loss: 2.443614 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 -> Train Loss: 2.417138 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 -> Train Loss: 2.384015 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 -> Train Loss: 2.373134 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 -> Train Loss: 2.349376 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 -> Train Loss: 2.329391 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 -> Train Loss: 2.316992 | Val MAP@7: 0.071326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 -> Train Loss: 2.295090 | Val MAP@7: 0.071326\n",
      "-> Loading best model for Fold 1 (MAP@7: 0.071326) for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== FOLD 2/5 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -> Train Loss: 3.435701 | Val MAP@7: 0.070240\n",
      "   -> 🎉 New best score! Model for Fold 2 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 -> Train Loss: 2.503355 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 -> Train Loss: 2.444811 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 -> Train Loss: 2.416897 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 -> Train Loss: 2.377411 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 -> Train Loss: 2.353336 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 -> Train Loss: 2.325461 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 -> Train Loss: 2.285412 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 -> Train Loss: 2.264787 | Val MAP@7: 0.070240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 -> Train Loss: 2.247444 | Val MAP@7: 0.070240\n",
      "-> Loading best model for Fold 2 (MAP@7: 0.070240) for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== FOLD 3/5 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -> Train Loss: 3.468970 | Val MAP@7: 0.072136\n",
      "   -> 🎉 New best score! Model for Fold 3 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 -> Train Loss: 2.520699 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 -> Train Loss: 2.473070 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 -> Train Loss: 2.432280 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 -> Train Loss: 2.406925 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 -> Train Loss: 2.394448 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 -> Train Loss: 2.364064 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 -> Train Loss: 2.352107 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 -> Train Loss: 2.330311 | Val MAP@7: 0.072136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 -> Train Loss: 2.322518 | Val MAP@7: 0.072136\n",
      "-> Loading best model for Fold 3 (MAP@7: 0.072136) for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== FOLD 4/5 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -> Train Loss: 3.656245 | Val MAP@7: 0.070717\n",
      "   -> 🎉 New best score! Model for Fold 4 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 -> Train Loss: 2.506965 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 -> Train Loss: 2.457231 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 -> Train Loss: 2.427223 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 -> Train Loss: 2.389433 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 -> Train Loss: 2.374139 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 -> Train Loss: 2.355291 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 -> Train Loss: 2.337255 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 -> Train Loss: 2.317742 | Val MAP@7: 0.070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 -> Train Loss: 2.298141 | Val MAP@7: 0.070717\n",
      "-> Loading best model for Fold 4 (MAP@7: 0.070717) for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== FOLD 5/5 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -> Train Loss: 3.614066 | Val MAP@7: 0.072770\n",
      "   -> 🎉 New best score! Model for Fold 5 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 -> Train Loss: 2.510343 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 -> Train Loss: 2.454408 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 -> Train Loss: 2.418739 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 -> Train Loss: 2.393563 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 -> Train Loss: 2.377550 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 -> Train Loss: 2.365309 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 -> Train Loss: 2.351630 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 -> Train Loss: 2.336400 | Val MAP@7: 0.072770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16518/2002978760.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 -> Train Loss: 2.322427 | Val MAP@7: 0.072770\n",
      "-> Loading best model for Fold 5 (MAP@7: 0.072770) for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- K-Fold Training Finished. Saving final OOF and Test predictions. ---\n",
      "✅ Saved 'train_4_ensemble.parquet' with OOF residual predictions.\n",
      "✅ Saved 'test_4_ensemble.parquet' with averaged test residual predictions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "# =================================================================================\n",
    "# 🚀 SCRIPT 2: TRAIN RESIDUAL TRANSFORMER\n",
    "# =================================================================================\n",
    "# This script trains an advanced Transformer model. Instead of predicting the\n",
    "# target directly, it's trained to predict the *error* (residual) of the\n",
    "# primary LGBM model. This allows it to correct the base model's mistakes.\n",
    "\n",
    "# --- Competition Metric Calculation ---\n",
    "def calculate_map_at_7(y_true, y_pred_probs, ids_df):\n",
    "    \"\"\"Calculates Mean Average Precision at 7 for the Amex competition.\"\"\"\n",
    "    \n",
    "    def ap_at_k(group, k=7):\n",
    "        \"\"\"Calculates Average Precision at k for a single group.\"\"\"\n",
    "        group = group.sort_values('pred', ascending=False)\n",
    "        y_true_sorted = group['y'].values\n",
    "        \n",
    "        # Truncate at k\n",
    "        y_true_sorted = y_true_sorted[:k]\n",
    "        \n",
    "        relevance = (y_true_sorted == 1)\n",
    "        if np.sum(relevance) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        precision_at_i = np.cumsum(relevance) / (np.arange(len(relevance)) + 1)\n",
    "        ap = np.sum(precision_at_i * relevance) / np.sum(relevance)\n",
    "        return ap\n",
    "\n",
    "    # Create a temporary DataFrame for calculation\n",
    "    calc_df = ids_df[['id2', 'id5']].copy()\n",
    "    calc_df['y'] = y_true\n",
    "    calc_df['pred'] = y_pred_probs\n",
    "\n",
    "    # --- SCORING SCRIPT FIX ---\n",
    "    # Convert id5 to date to match official scoring logic\n",
    "    calc_df['id5'] = pd.to_datetime(calc_df['id5']).dt.date\n",
    "    \n",
    "    # Calculate AP for each customer-session group\n",
    "    ap_scores = calc_df.groupby(['id2', 'id5']).apply(ap_at_k)\n",
    "    \n",
    "    # Return the mean of the AP scores\n",
    "    return ap_scores.mean()\n",
    "\n",
    "# --- Advanced Transformer Configuration ---\n",
    "class TransformerConfig:\n",
    "    TRAIN_FILE = \"inter/train_3_oof.parquet\"\n",
    "    TEST_FILE = \"inter/test_3_oof.parquet\"\n",
    "    DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "    \n",
    "    CAT_EMBED_DIM = 8\n",
    "    EMBED_DIM = 192\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 4\n",
    "    DIM_FEEDFORWARD = 512\n",
    "    DROPOUT = 0.2\n",
    "    MAX_SEQ_LEN = 20\n",
    "    \n",
    "    EPOCHS = 10\n",
    "    N_SPLITS = 5\n",
    "    LR = 5e-5\n",
    "    BATCH_SIZE = 256\n",
    "    NUM_WORKERS = 2\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    MODEL_OUTPUT_PATH = \"model/transformer_v2_final_model\"\n",
    "    ENSEMBLE_WEIGHT = 0.1 ############### CRITICAL FIX ###############\n",
    "\n",
    "# --- PyTorch Dataset and Model ---\n",
    "class AmexAdvancedDataset(Dataset):\n",
    "    def __init__(self, sequences_cat, sequences_num, labels):\n",
    "        self.sequences_cat = torch.tensor(sequences_cat, dtype=torch.long)\n",
    "        self.sequences_num = torch.tensor(sequences_num, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences_cat[idx], self.sequences_num[idx], self.labels[idx]\n",
    "\n",
    "class AmexPureTransformer(nn.Module):\n",
    "    def __init__(self, config, num_numerical_features, cat_cardinalities):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, config.CAT_EMBED_DIM) for cardinality in cat_cardinalities\n",
    "        ])\n",
    "        total_cat_embed_dim = len(cat_cardinalities) * config.CAT_EMBED_DIM\n",
    "        self.num_norm = nn.LayerNorm(num_numerical_features)\n",
    "        combined_feature_dim = total_cat_embed_dim + num_numerical_features\n",
    "        self.feature_proj = nn.Linear(combined_feature_dim, config.EMBED_DIM)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, config.MAX_SEQ_LEN, config.EMBED_DIM))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.EMBED_DIM, nhead=config.NUM_HEADS,\n",
    "            dim_feedforward=config.DIM_FEEDFORWARD, dropout=config.DROPOUT,\n",
    "            activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.NUM_LAYERS)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(config.EMBED_DIM),\n",
    "            nn.Linear(config.EMBED_DIM, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        cat_embeds = [self.cat_embeddings[i](x_cat[:, :, i]) for i in range(x_cat.size(2))]\n",
    "        all_cat_embeds = torch.cat(cat_embeds, dim=-1)\n",
    "        x_num_norm = self.num_norm(x_num)\n",
    "        x = torch.cat([all_cat_embeds, x_num_norm], dim=-1)\n",
    "        x = self.feature_proj(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.head(x[:, -1, :]) # Use last token for prediction\n",
    "        return x\n",
    "\n",
    "# --- Data Preparation and Training Functions ---\n",
    "def prepare_advanced_transformer_data(config):\n",
    "    print(\"\\n--- Preparing data for Advanced Transformer (Residual Modeling) ---\")\n",
    "    \n",
    "    train_df = pd.read_parquet(config.TRAIN_FILE)\n",
    "    test_df = pd.read_parquet(config.TEST_FILE)\n",
    "    \n",
    "    print(\"   -> Creating residual target (y - lgbm_oof_prediction)...\")\n",
    "    train_df['residual_target'] = train_df['y'] - train_df['oof_lgbm_prediction']\n",
    "    \n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    data_dict = pd.read_csv(config.DATA_DICT_PATH)\n",
    "    all_cats_from_dict = data_dict[data_dict[\"Type\"].str.strip() == \"Categorical\"][\"masked_column\"].tolist()\n",
    "    new_cat_features = [\n",
    "        'offer_category', 'session_id', 'time_of_day_bin', 'is_weekend',\n",
    "        'is_holiday_week', 'is_payday_week', 'offer_lifecycle_stage',\n",
    "        'brand_id', 'industry_id'\n",
    "    ]\n",
    "    categorical_cols = [col for col in train_df.columns if col in all_cats_from_dict + new_cat_features and col != 'id3']\n",
    "    numerical_cols = [col for col in train_df.columns if col.startswith('f') and col not in categorical_cols]\n",
    "\n",
    "    print(\"\\n   -> Attempting to load list of top features for Transformer...\")\n",
    "    selected_features_path = os.path.join(\"model\", \"top_100_features.json\")\n",
    "    try:\n",
    "        with open(selected_features_path, 'r') as f:\n",
    "            top_features = json.load(f)\n",
    "        print(f\"   -> ✅ Successfully loaded {len(top_features)} selected features from '{selected_features_path}'.\")\n",
    "        numerical_cols = [col for col in numerical_cols if col in top_features]\n",
    "        categorical_cols = [col for col in categorical_cols if col in top_features]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   -> ⚠️ WARNING: Feature selection file not found at '{selected_features_path}'.\")\n",
    "        print(\"   -> Proceeding with all available features for the Transformer model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ❌ ERROR: Could not read feature file. Error: {e}. Using all features.\")\n",
    "\n",
    "    if 'oof_lgbm_prediction' not in numerical_cols:\n",
    "        numerical_cols.append('oof_lgbm_prediction')\n",
    "\n",
    "    del combined_df; gc.collect()\n",
    "\n",
    "    cat_cardinalities = []\n",
    "    for col in tqdm(categorical_cols, desc=\"   -> Encoding Categoricals (Leakage-Proof)\"):\n",
    "        codes, uniques = pd.factorize(train_df[col], sort=True)\n",
    "        mapping = {val: i + 1 for i, val in enumerate(uniques)}\n",
    "        train_df[col] = codes + 1\n",
    "        test_df[col] = test_df[col].map(mapping).fillna(0).astype(int)\n",
    "        cat_cardinalities.append(len(uniques) + 1)\n",
    "\n",
    "    def create_sequences(df, cols_cat, cols_num, lbl_col=None, is_test=False):\n",
    "        sequences_cat, sequences_num, labels, customer_ids = [], [], [], []\n",
    "        grouped = df.groupby('id2')\n",
    "        for cust_id, group in tqdm(grouped, desc=f\"Creating sequences ({'test' if is_test else 'train'})\"):\n",
    "            cat_feats = group[cols_cat].values\n",
    "            num_feats = group[cols_num].values\n",
    "            \n",
    "            padding_len = config.MAX_SEQ_LEN - len(group)\n",
    "            if padding_len > 0:\n",
    "                cat_feats = np.vstack([np.zeros((padding_len, cat_feats.shape[1])), cat_feats])\n",
    "                num_feats = np.vstack([np.zeros((padding_len, num_feats.shape[1])), num_feats])\n",
    "            else:\n",
    "                cat_feats = cat_feats[-config.MAX_SEQ_LEN:]\n",
    "                num_feats = num_feats[-config.MAX_SEQ_LEN:]\n",
    "            \n",
    "            sequences_cat.append(cat_feats)\n",
    "            sequences_num.append(num_feats)\n",
    "            customer_ids.append(cust_id)\n",
    "            if not is_test:\n",
    "                labels.append(group[lbl_col].iloc[-1])\n",
    "        \n",
    "        final_cat = np.array(sequences_cat)\n",
    "        final_num = np.nan_to_num(np.array(sequences_num))\n",
    "        \n",
    "        if is_test:\n",
    "            return final_cat, final_num, np.array(customer_ids)\n",
    "        return final_cat, final_num, np.array(labels), np.array(customer_ids)\n",
    "\n",
    "    cat_train, num_train, lbl_train, train_cust_ids = create_sequences(train_df, categorical_cols, numerical_cols, 'residual_target')\n",
    "    cat_test, num_test, test_cust_ids = create_sequences(test_df, categorical_cols, numerical_cols, is_test=True)\n",
    "    \n",
    "    train_data = (cat_train, num_train, lbl_train, train_cust_ids)\n",
    "    test_data = (cat_test, num_test, test_cust_ids)\n",
    "    \n",
    "    return train_data, test_data, len(numerical_cols), cat_cardinalities\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        cat_seq, num_seq, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cat_seq, num_seq).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler: scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device, ids_df, lgbm_oof_preds_fold, val_customer_ids):\n",
    "    \"\"\"\n",
    "    Evaluates the transformer model on a validation fold.\n",
    "    It combines the base LGBM OOF preds with the transformer's residual preds.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get residual predictions from the transformer (one prediction per customer)\n",
    "    residual_preds = predict_transformer(model, dataloader, device)\n",
    "    \n",
    "    # --- FIX for length mismatch: Broadcast customer-level preds to event-level ---\n",
    "    # Create a mapping from each customer ID to their single predicted residual\n",
    "    customer_to_residual_map = pd.Series(residual_preds, index=val_customer_ids)\n",
    "    \n",
    "    # Use the map to assign the correct residual to every event row\n",
    "    eval_df = ids_df[ids_df['id2'].isin(val_customer_ids)].copy()\n",
    "    eval_df['residual_pred'] = eval_df['id2'].map(customer_to_residual_map)\n",
    "    # --- END FIX ---\n",
    "\n",
    "    # Combine with base LGBM OOF predictions\n",
    "    eval_df['lgbm_pred'] = lgbm_oof_preds_fold\n",
    "    \n",
    "    # Ensure all components have the same length and are aligned\n",
    "    # This is a critical sanity check that should now pass\n",
    "    if len(eval_df) != len(lgbm_oof_preds_fold):\n",
    "        print(\"Length mismatch during evaluation! This should not happen.\")\n",
    "        print(f\"Eval DF: {len(eval_df)}, LGBM preds: {len(lgbm_oof_preds_fold)}\")\n",
    "        # Attempt to align, assuming the dataloader was created from a sorted df\n",
    "        eval_df = eval_df.sort_values(['id2', 'id5', 'id4']).reset_index(drop=True)\n",
    "        if len(eval_df) != len(lgbm_oof_preds_fold):\n",
    "             # If it still doesn't match, we cannot proceed with evaluation for this fold\n",
    "             return 0.0\n",
    "\n",
    "    # CRITICAL FIX: Add a weight to the residual prediction to prevent it from overwhelming the base model\n",
    "    config = TransformerConfig()\n",
    "    eval_df['final_pred'] = eval_df['lgbm_pred'] + config.ENSEMBLE_WEIGHT * eval_df['residual_pred']\n",
    "    \n",
    "    # Calculate MAP@7 on the final ensembled prediction\n",
    "    score = calculate_map_at_7(eval_df['y'], eval_df['final_pred'], eval_df[['id2', 'id5']])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def predict_transformer(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
    "            outputs = model(batch[0].to(device), batch[1].to(device))\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(predictions).flatten()\n",
    "\n",
    "def run_stage2_transformer():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 2: RESIDUAL TRANSFORMER PIPELINE ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    config = TransformerConfig()\n",
    "    (train_data, test_data, num_numerical, cat_cards) = prepare_advanced_transformer_data(config)\n",
    "    \n",
    "    cat_train_full, num_train_full, lbl_train_full, train_cust_ids = train_data\n",
    "    cat_test, num_test, test_cust_ids = test_data\n",
    "    \n",
    "    original_train_df = pd.read_parquet(config.TRAIN_FILE)\n",
    "    customer_y_stratify = original_train_df.groupby('id2')['y'].max()\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config.N_SPLITS, shuffle=True, random_state=42)\n",
    "    oof_residual_preds = np.zeros(len(cat_train_full))\n",
    "    test_residual_preds = np.zeros(len(cat_test))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(customer_y_stratify)), customer_y_stratify)):\n",
    "        print(f\"\\n{'='*30} FOLD {fold+1}/{config.N_SPLITS} {'='*30}\")\n",
    "        \n",
    "        X_cat_train, X_num_train, y_res_train = cat_train_full[train_idx], num_train_full[train_idx], lbl_train_full[train_idx]\n",
    "        X_cat_val, X_num_val, y_res_val = cat_train_full[val_idx], num_train_full[val_idx], lbl_train_full[val_idx]\n",
    "        \n",
    "        val_cust_ids = customer_y_stratify.index[val_idx]\n",
    "        ids_val_fold_df = original_train_df[original_train_df['id2'].isin(val_cust_ids)].copy()\n",
    "        lgbm_oof_val_fold = ids_val_fold_df['oof_lgbm_prediction'].values\n",
    "\n",
    "        loader_train = DataLoader(AmexAdvancedDataset(X_cat_train, X_num_train, y_res_train), batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "        loader_val = DataLoader(AmexAdvancedDataset(X_cat_val, X_num_val, y_res_val), batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = AmexPureTransformer(config, num_numerical, cat_cards).to(config.DEVICE)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=1e-2)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=2)\n",
    "\n",
    "        best_map_score = -1.0\n",
    "        for epoch in range(config.EPOCHS):\n",
    "            train_loss = train_one_epoch(model, loader_train, optimizer, criterion, config.DEVICE)\n",
    "            map_score = evaluate_model(model, loader_val, config.DEVICE, ids_val_fold_df, lgbm_oof_val_fold, val_cust_ids)\n",
    "            print(f\"Epoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_loss:.6f} | Val MAP@7: {map_score:.6f}\")\n",
    "            scheduler.step(map_score)\n",
    "            if map_score > best_map_score:\n",
    "                best_map_score = map_score\n",
    "                torch.save(model.state_dict(), f\"{config.MODEL_OUTPUT_PATH}_fold_{fold+1}.pth\")\n",
    "                print(f\"   -> 🎉 New best score! Model for Fold {fold+1} saved.\")\n",
    "        \n",
    "        print(f\"-> Loading best model for Fold {fold+1} (MAP@7: {best_map_score:.6f}) for predictions...\")\n",
    "        model.load_state_dict(torch.load(f\"{config.MODEL_OUTPUT_PATH}_fold_{fold+1}.pth\"))\n",
    "        \n",
    "        val_preds_fold = predict_transformer(model, loader_val, config.DEVICE)\n",
    "        oof_residual_preds[val_idx] = val_preds_fold\n",
    "        \n",
    "        test_loader = DataLoader(AmexAdvancedDataset(cat_test, num_test, np.zeros(len(cat_test))), batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "        test_preds_fold = predict_transformer(model, test_loader, config.DEVICE)\n",
    "        test_residual_preds += test_preds_fold / config.N_SPLITS\n",
    "        \n",
    "        del model, loader_train, loader_val; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n--- K-Fold Training Finished. Saving final OOF and Test predictions. ---\")\n",
    "    \n",
    "    # --- FIX: Save OOF predictions for validation in the next stage ---\n",
    "    oof_map = pd.DataFrame({'id2': train_cust_ids, 'oof_transformer_residual': oof_residual_preds})\n",
    "    original_train_df = original_train_df.merge(oof_map, on='id2', how='left')\n",
    "    original_train_df.to_parquet(\"inter/train_4_ensemble.parquet\", index=False)\n",
    "    print(\"✅ Saved 'train_4_ensemble.parquet' with OOF residual predictions.\")\n",
    "    \n",
    "    test_map = pd.DataFrame({'id2': test_cust_ids, 'transformer_residual': test_residual_preds})\n",
    "    test_df = pd.read_parquet(config.TEST_FILE)\n",
    "    test_df = test_df.merge(test_map, on='id2', how='left')\n",
    "    test_df.to_parquet(\"inter/test_4_ensemble.parquet\", index=False)\n",
    "    print(\"✅ Saved 'test_4_ensemble.parquet' with averaged test residual predictions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # STEP 2: Train the Transformer to predict the residuals of the LGBM model.\n",
    "    run_stage2_transformer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a341ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 === SCRIPT 3: FINAL RESIDUAL ENSEMBLING & SUBMISSION ===\n",
      "================================================================================\n",
      "\n",
      "--- Applying final combination to test set predictions ---\n",
      "   -> Scaling final predictions to the [0, 1] range for submission.\n",
      "\n",
      "--- Creating final submission file ---\n",
      "\n",
      "🎉🎉 Final ensemble submission file created successfully: 'final_residual_ensemble_submission.csv' 🎉🎉\n",
      "Submission Head:\n",
      "                                            id1      id2     id3         id5  \\\n",
      "0   1000064_25530_16-23_2023-11-06 12:27:38.171  1000064   25530  2023-11-06   \n",
      "1   1000064_64401_16-23_2023-11-06 12:27:48.533  1000064   64401  2023-11-06   \n",
      "2  1000064_260067_16-23_2023-11-06 12:28:22.141  1000064  260067  2023-11-06   \n",
      "3  1000064_989989_16-23_2023-11-06 12:28:22.267  1000064  989989  2023-11-06   \n",
      "4   1000064_521695_16-23_2023-11-06 12:28:22.85  1000064  521695  2023-11-06   \n",
      "\n",
      "       pred  \n",
      "0  0.624095  \n",
      "1  0.646328  \n",
      "2  0.593941  \n",
      "3  0.125083  \n",
      "4  0.285315  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This is a placeholder for the TransformerConfig class. \n",
    "# In your notebook, this class should already be defined in a previous cell.\n",
    "class TransformerConfig:\n",
    "    ENSEMBLE_WEIGHT = 0.1\n",
    "\n",
    "# =================================================================================\n",
    "# 🚀 SCRIPT 3: FINAL ENSEMBLING AND SUBMISSION\n",
    "# =================================================================================\n",
    "# This final script combines the base LGBM predictions with the Transformer's\n",
    "# residual predictions. It validates the approach on OOF data, applies the\n",
    "# combination to the test set, scales the results, and generates the final\n",
    "# submission file.\n",
    "\n",
    "def run_final_ensembling():\n",
    "    \"\"\"\n",
    "    Combines the base LGBM predictions with the averaged Transformer residual\n",
    "    predictions to create the final submission file.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 3: FINAL RESIDUAL ENSEMBLING & SUBMISSION ===\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Load base predictions and residual predictions\n",
    "    # Make sure these files exist from the previous steps\n",
    "    try:\n",
    "        lgbm_preds = pd.read_csv('lgbm_test_preds.csv')\n",
    "        transformer_preds_full = pd.read_parquet('inter/test_4_ensemble.parquet')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Could not find required input file. Make sure previous steps ran successfully.\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # --- FIX: Select only necessary columns to prevent column name clashes during merge ---\n",
    "    transformer_preds = transformer_preds_full[['id1', 'transformer_residual']]\n",
    "\n",
    "    # Merge predictions\n",
    "    print(\"--- Applying final combination to test set predictions ---\")\n",
    "    merged_df = lgbm_preds.merge(transformer_preds, on='id1')\n",
    "\n",
    "    # Ensemble with a weight on the residual\n",
    "    config = TransformerConfig()\n",
    "    merged_df['final_pred'] = merged_df['pred'] + config.ENSEMBLE_WEIGHT * merged_df['transformer_residual']\n",
    "\n",
    "    # Scale final predictions to [0, 1] for submission\n",
    "    print(\"   -> Scaling final predictions to the [0, 1] range for submission.\")\n",
    "    final_pred = merged_df['final_pred'].values\n",
    "    final_pred_scaled = (final_pred - final_pred.min()) / (final_pred.max() - final_pred.min())\n",
    "    merged_df['pred'] = final_pred_scaled\n",
    "\n",
    "    # --- 4. Create Final Submission File ---\n",
    "    print(\"\\n--- Creating final submission file ---\")\n",
    "    \n",
    "    # --- FIX: Use the 'merged_df' which now contains all necessary columns ---\n",
    "    # Reorder columns to exactly match the submission template.\n",
    "    submission_df = merged_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "    \n",
    "    # --- SCORING SCRIPT FIX ---\n",
    "    # The submission requires id5 (as date).\n",
    "    submission_df['id5'] = pd.to_datetime(submission_df['id5']).dt.date\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = 'final_residual_ensemble_submission.csv'\n",
    "    # --- FIX: Save the correct DataFrame ('submission_df') ---\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n🎉🎉 Final ensemble submission file created successfully: '{output_path}' 🎉🎉\")\n",
    "    print(\"Submission Head:\")\n",
    "    # --- FIX: Print the head of the correct DataFrame ---\n",
    "    print(submission_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # STEP 3: Ensemble the predictions, validate, and create the final submission.\n",
    "    run_final_ensembling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fff0d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 === SCRIPT 4: Generating Feature Importance Report ===\n",
      "================================================================================\n",
      "\n",
      "   -> Successfully loaded final trained model.\n",
      "   -> Extracted importance for 3093 features.\n",
      "\n",
      "✅ Feature importance report saved successfully to 'feature_importance.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def generate_feature_importance_report():\n",
    "    \"\"\"\n",
    "    Loads the final trained model, extracts feature importances,\n",
    "    and saves them to an Excel file as required for submission.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 4: Generating Feature Importance Report ===\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load the final trained model\n",
    "        model = lgb.Booster(model_file=\"model/lgbm_final_ranker_model.txt\")\n",
    "        print(\"   -> Successfully loaded final trained model.\")\n",
    "        \n",
    "        # Extract feature importance\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': model.feature_name(),\n",
    "            'importance_gain': model.feature_importance(importance_type='gain'),\n",
    "            'importance_split': model.feature_importance(importance_type='split')\n",
    "        }).sort_values(by='importance_gain', ascending=False)\n",
    "        \n",
    "        print(f\"   -> Extracted importance for {len(importance_df)} features.\")\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_path = 'feature_importance.xlsx'\n",
    "        importance_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "        \n",
    "        print(f\"\\n✅ Feature importance report saved successfully to '{output_path}'\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Error: Could not find the final model file 'model/lgbm_final_ranker_model.txt'.\")\n",
    "        print(\"   -> Please ensure the model has been trained and saved before running this step.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # STEP 4: Generate the feature importance report for submission.\n",
    "    generate_feature_importance_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a20e9b-953d-4163-876a-2b226c8370ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
